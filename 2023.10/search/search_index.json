{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Course website Teachers Geert van Geest .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Antonin Thi\u00e9baut .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Damir Zhakparov Authors Geert van Geest .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Antonin Thi\u00e9baut .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Patricia Palagi .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Attribution This course is partly inspired by the Carpentries Docker course and the official Snakemake tutorial . License & copyright License: CC BY-SA 4.0 Copyright: SIB Swiss Institute of Bioinformatics Material This website Google doc (through mail) Learning outcomes General learning outcomes After this course, you will be able to: Understand the basic concepts and terminology associated with virtualization with containers Customize, store, manage and share containerized environments with Docker Use Singularity to run containers on a shared computer environment (e.g. a HPC cluster) Understand the basic concepts and terminology associated with workflow management systems Create a computational workflow that uses containers and package managers with Snakemake Learning outcomes explained To reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter (found at Course material ) starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn. Learning experiences To reach the learning outcomes we will use lectures, exercises, polls and group work. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only. Exercises Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. All answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different. Asking questions During lectures, you are encouraged to raise your hand if you have questions. A main source of communication will be our slack channel . Ask background questions that interest you personally at #background . During the exercises, e.g. if you are stuck or don\u2019t understand what is going on, use the slack channel #q-and-a . This channel is not only meant for asking questions but also for answering questions of other participants. If you are replying to a question, use the \u201creply in thread\u201d option: The teachers will review the answers, and add/modify if necessary. If you\u2019re really stuck and need specific tutor support, write the teachers or helpers personally. To summarise: During lectures: raise hand Personal interest questions: #background on slack During exercises: raise hand/ #q-and-a on slack","title":"Home"},{"location":"#course-website","text":"","title":"Course website"},{"location":"#teachers","text":"Geert van Geest .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Antonin Thi\u00e9baut .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Damir Zhakparov","title":"Teachers"},{"location":"#authors","text":"Geert van Geest .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Antonin Thi\u00e9baut .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Patricia Palagi .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;}","title":"Authors"},{"location":"#attribution","text":"This course is partly inspired by the Carpentries Docker course and the official Snakemake tutorial .","title":"Attribution"},{"location":"#license-copyright","text":"License: CC BY-SA 4.0 Copyright: SIB Swiss Institute of Bioinformatics","title":"License &amp; copyright"},{"location":"#material","text":"This website Google doc (through mail)","title":"Material"},{"location":"#learning-outcomes","text":"","title":"Learning outcomes"},{"location":"#general-learning-outcomes","text":"After this course, you will be able to: Understand the basic concepts and terminology associated with virtualization with containers Customize, store, manage and share containerized environments with Docker Use Singularity to run containers on a shared computer environment (e.g. a HPC cluster) Understand the basic concepts and terminology associated with workflow management systems Create a computational workflow that uses containers and package managers with Snakemake","title":"General learning outcomes"},{"location":"#learning-outcomes-explained","text":"To reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter (found at Course material ) starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn.","title":"Learning outcomes explained"},{"location":"#learning-experiences","text":"To reach the learning outcomes we will use lectures, exercises, polls and group work. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.","title":"Learning experiences"},{"location":"#exercises","text":"Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. All answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different.","title":"Exercises"},{"location":"#asking-questions","text":"During lectures, you are encouraged to raise your hand if you have questions. A main source of communication will be our slack channel . Ask background questions that interest you personally at #background . During the exercises, e.g. if you are stuck or don\u2019t understand what is going on, use the slack channel #q-and-a . This channel is not only meant for asking questions but also for answering questions of other participants. If you are replying to a question, use the \u201creply in thread\u201d option: The teachers will review the answers, and add/modify if necessary. If you\u2019re really stuck and need specific tutor support, write the teachers or helpers personally. To summarise: During lectures: raise hand Personal interest questions: #background on slack During exercises: raise hand/ #q-and-a on slack","title":"Asking questions"},{"location":"course_schedule/","text":"Day 1 - Containers Block Start End subject Block 1 9:00 AM 10:30 AM Introduction to containers 10:30 AM 11:00 AM BREAK Block 2 11:00 AM 12:30 PM Managing containers and images 12:30 PM 1:30 PM BREAK Block 3 1:30 PM 3:00 PM Working with dockerfiles 3:00 PM 3:30 PM BREAK Block 4 3:30 PM 5:00 PM Running containers with singularity Day 2 - Snakemake Block Start End subject Block 1 9:00 AM 10:30 AM Introduction to Snakemake 10:30 AM 11:00 AM BREAK Block 2 11:00 AM 12:30 PM Advanced Snakemake 12:30 PM 1:30 PM BREAK Block 3 1:30 PM 3:00 PM Snakemake, package managers and containers 3:00 PM 3:30 PM BREAK Block 4 3:30 PM 4:30 PM Whole workflow 4:30 PM 5:00 PM Wrap-up & Open Q&A","title":"Course schedule"},{"location":"course_schedule/#day-1-containers","text":"Block Start End subject Block 1 9:00 AM 10:30 AM Introduction to containers 10:30 AM 11:00 AM BREAK Block 2 11:00 AM 12:30 PM Managing containers and images 12:30 PM 1:30 PM BREAK Block 3 1:30 PM 3:00 PM Working with dockerfiles 3:00 PM 3:30 PM BREAK Block 4 3:30 PM 5:00 PM Running containers with singularity","title":"Day 1 - Containers"},{"location":"course_schedule/#day-2-snakemake","text":"Block Start End subject Block 1 9:00 AM 10:30 AM Introduction to Snakemake 10:30 AM 11:00 AM BREAK Block 2 11:00 AM 12:30 PM Advanced Snakemake 12:30 PM 1:30 PM BREAK Block 3 1:30 PM 3:00 PM Snakemake, package managers and containers 3:00 PM 3:30 PM BREAK Block 4 3:30 PM 4:30 PM Whole workflow 4:30 PM 5:00 PM Wrap-up & Open Q&A","title":"Day 2 - Snakemake"},{"location":"precourse/","text":"UNIX As is stated in the course prerequisites at the announcement web page . We expect participants to have a basic understanding of working with the command line on UNIX-based systems. You can test your UNIX skills with a quiz here . If you don\u2019t have experience with UNIX command line, or if you\u2019re unsure whether you meet the prerequisites, follow our online UNIX tutorial . Software Install Docker on your local computer and create an account on dockerhub . You can find instructions here . Note that you need admin rights to install and use Docker, and if you are installing Docker on Windows, you need a recent Windows version. You should also have a modern code editor installed, like Sublime Text or VScode . If working with Windows During the course exercises you will be mainly interacting with docker through the command line. Although windows powershell is suitable for that, it is easier to follow the exercises if you have UNIX or \u2018UNIX-like\u2019 terminal. You can get this by using MobaXterm or WSL2 . Make sure you install the latest versions before installing docker. If installing Docker is a problem During the course, we can give only limited support for installation issues. If you do not manage to install Docker before the course, you can still do almost all exercises on Play with Docker . A Docker login is required. In addition to your local computer, we will be working on an Amazon Web Services ( AWS ) Elastic Cloud (EC2) server. Our Ubuntu server behaves like a \u2018normal\u2019 remote server, and can be approached through ssh with a username, key and IP address. All participants will be granted access to a personal home directory.","title":"Precourse preparations"},{"location":"precourse/#unix","text":"As is stated in the course prerequisites at the announcement web page . We expect participants to have a basic understanding of working with the command line on UNIX-based systems. You can test your UNIX skills with a quiz here . If you don\u2019t have experience with UNIX command line, or if you\u2019re unsure whether you meet the prerequisites, follow our online UNIX tutorial .","title":"UNIX"},{"location":"precourse/#software","text":"Install Docker on your local computer and create an account on dockerhub . You can find instructions here . Note that you need admin rights to install and use Docker, and if you are installing Docker on Windows, you need a recent Windows version. You should also have a modern code editor installed, like Sublime Text or VScode . If working with Windows During the course exercises you will be mainly interacting with docker through the command line. Although windows powershell is suitable for that, it is easier to follow the exercises if you have UNIX or \u2018UNIX-like\u2019 terminal. You can get this by using MobaXterm or WSL2 . Make sure you install the latest versions before installing docker. If installing Docker is a problem During the course, we can give only limited support for installation issues. If you do not manage to install Docker before the course, you can still do almost all exercises on Play with Docker . A Docker login is required. In addition to your local computer, we will be working on an Amazon Web Services ( AWS ) Elastic Cloud (EC2) server. Our Ubuntu server behaves like a \u2018normal\u2019 remote server, and can be approached through ssh with a username, key and IP address. All participants will be granted access to a personal home directory.","title":"Software"},{"location":"course_material/dockerfiles/","text":"Learning outcomes After having completed this chapter you will be able to: Build an image based on a dockerfile Use the basic dockerfile syntax Change the default command of an image and validate the change Map ports to a container to display interactive content through a browser Material Official Dockerfile reference Ten simple rules for writing dockerfiles Exercises To make your images shareable and adjustable, it\u2019s good practice to work with a Dockerfile . This is a script with a set of instructions to build your image from an existing image. Basic Dockerfile You can generate an image from a Dockerfile using the command docker build . A Dockerfile has its own syntax for giving instructions. Luckily, they are rather simple. The script always contains a line starting with FROM that takes the image name from which the new image will be built. After that you usually want to run some commands to e.g. configure and/or install software. The instruction to run these commands during building starts with RUN . In our figlet example that would be: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet On writing reproducible Dockerfiles At the FROM statement in the above Dockerfile you see that we have added a specific tag to the image (i.e. jammy-20230308 ). We could also have written: FROM ubuntu RUN apt-get update RUN apt-get install figlet This will automatically pull the image with the tag latest . However, if the maintainer of the ubuntu images decides to tag another ubuntu version as latest , rebuilding with the above Dockerfile will not give you the same result. Therefore it\u2019s always good practice to add the (stable) tag to the image in a Dockerfile . More rules on making your Dockerfiles more reproducible here . Exercise: Create a file on your computer called Dockerfile , and paste the above instruction lines in that file. Make the directory containing the Dockerfile your current directory. Build a new image based on that Dockerfile with: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build . docker build --platform amd64 . If using an Apple M1 chip (newer Macs) If you are using a computer with an Apple M1 chip, you have the less common ARM system architecture, which can limit transferability of images to (more common) x86_64/AMD64 machines. When building images on a Mac with an M1 chip (especially if you have sharing in mind), it\u2019s best to specify the --platform amd64 flag. The argument of docker build The command docker build takes a directory as input (providing . means the current directory). This directory should contain the Dockerfile , but it can also contain more of the build context, e.g. (python, R, shell) scripts that are required to build the image. What has happened? What is the name of the build image? Answer A new image was created based on the Dockerfile . You can check it with: docker image ls , which gives something like: REPOSITORY TAG IMAGE ID CREATED SIZE <none> <none> 92c980b09aad 7 seconds ago 101MB ubuntu-figlet latest e08b999c7978 About an hour ago 101MB ubuntu latest f63181f19b2f 30 hours ago 72.9MB It has created an image without a name or tag. That\u2019s a bit inconvenient. Exercise: Build a new image with a specific name. You can do that with adding the option -t to docker build . Before that, remove the nameless image. Hint An image without a name is usually a \u201cdangling image\u201d. You can remove those with docker image prune . Answer Remove the nameless image with docker image prune . After that, rebuild an image with a name: x86_64 / AMD64 ARM (MacOS M1 chip) docker build -t ubuntu-figlet:v2 . docker build --platform amd64 -t ubuntu-figlet:v2 . Using CMD As you might remember the second positional argument of docker run is a command (i.e. docker run IMAGE [CMD] ). If you leave it empty, it uses the default command. You can change the default command in the Dockerfile with an instruction starting with CMD . For example: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD figlet My image works! Exercise: Build a new image based on the above Dockerfile . Can you validate the change using docker image inspect ? Can you overwrite this default with docker run ? Answer Copy the new line to your Dockerfile , and build the new image like this: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t ubuntu-figlet:v3 . docker build --platform amd64 -t ubuntu-figlet:v3 . The command docker inspect ubuntu-figlet:v3 will give: \"Cmd\": [ \"/bin/sh\", \"-c\", \"figlet My image works!\" ] So the default command ( /bin/bash ) has changed to figlet My image works! Running the image (with clean-up ( --rm )): docker run --rm ubuntu-figlet:v3 Will result in: __ __ _ _ _ | \\/ |_ _ (_)_ __ ___ __ _ __ _ ___ __ _____ _ __| | _____| | | |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| | | | | | |_| | | | | | | | | (_| | (_| | __/ \\ V V / (_) | | | <\\__ \\_| |_| |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___| \\_/\\_/ \\___/|_| |_|\\_\\___(_) |___/ |___/ And of course you can overwrite the default command: docker run --rm ubuntu-figlet:v3 figlet another text Resulting in: _ _ _ _ __ _ _ __ ___ | |_| |__ ___ _ __ | |_ _____ _| |_ / _` | '_ \\ / _ \\| __| '_ \\ / _ \\ '__| | __/ _ \\ \\/ / __| | (_| | | | | (_) | |_| | | | __/ | | || __/> <| |_ \\__,_|_| |_|\\___/ \\__|_| |_|\\___|_| \\__\\___/_/\\_\\\\__| Two flavours of CMD You have seen in the output of docker inspect that docker translates the command (i.e. figlet \"my image works!\" ) into this: [\"/bin/sh\", \"-c\", \"figlet 'My image works!'\"] . The notation we used in the Dockerfile is the shell notation while the notation with the square brackets ( [] ) is the exec-notation . You can use both notations in your Dockerfile . Altough the shell notation is more readable, the exec notation is directly used by the image, and therefore less ambiguous. A Dockerfile with shell notation: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD figlet My image works! A Dockerfile with exec notation: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD [ \"/bin/sh\" , \"-c\" , \"figlet My image works!\" ] Exercise: Now push our created image (with a version tag) to docker hub. We will use it later for the singularity exercises . Answer docker tag ubuntu-figlet:v3 [ USER NAME ] /ubuntu-figlet:v3 docker push [ USER NAME ] /ubuntu-figlet:v3 Build an image for your own script Often containers are built for a specific purpose. For example, you can use a container to ship all dependencies together with your developed set of scripts/programs. For that you will need to add your scripts to the container. That is quite easily done with the instruction COPY . However, in order to make your container more user-friendly, there are several additional instructions that can come in useful. We will treat the most frequently used ones below. Depending on your preference, either choose R or Python below. In the exercises will use a script called test_deseq2.R . This script will: Load the DESeq2 and optparse packages Load some additional packages to test their installations. We will use those packages later on in the course. Create and parse an option called --rows with optparse Create a dummy count matrix Run DESeq2 on the dummy count matrix Print the results to stdout You can download it here , or copy-paste it: test_deseq2.R #!/usr/bin/env Rscript # load packages required for this script write ( \"Loading packages required for this script\" , stderr ()) suppressPackageStartupMessages ({ library ( DESeq2 ) library ( optparse ) }) # load dependency packages for testing installations write ( \"Loading dependency packages for testing installations\" , stderr ()) suppressPackageStartupMessages ({ library ( apeglm ) library ( IHW ) library ( limma ) library ( data.table ) library ( ggplot2 ) library ( ggrepel ) library ( pheatmap ) library ( RColorBrewer ) library ( scales ) library ( stringr ) }) # parse options with optparse option_list <- list ( make_option ( c ( \"--rows\" ), type = \"integer\" , help = \"Number of rows in dummy matrix [default = %default]\" , default = 100 ) ) opt_parser <- OptionParser ( option_list = option_list , description = \"Runs DESeq2 on dummy data\" ) opt <- parse_args ( opt_parser ) # create a random dummy count matrix cnts <- matrix ( rnbinom ( n = opt $ row * 10 , mu = 100 , size = 1 / 0.5 ), ncol = 10 ) cond <- factor ( rep ( 1 : 2 , each = 5 )) # object construction dds <- DESeqDataSetFromMatrix ( cnts , DataFrame ( cond ), ~ cond ) # standard analysis dds <- DESeq ( dds ) res <- results ( dds ) # print results to stdout print ( res ) After you have downloaded it, make sure to set the permissions to executable: chmod +x test_deseq2.R It is a relatively simple script that runs DESeq2 on a dummy dataset. An example for execution would be: ./test_deseq2.R --rows 100 Here, --rows is a optional arguments that specifies the number of rows generated in the input count matrix. When running the script, it will return a bunch of messages and at the end an overview of differential gene expression analysis results: baseMean log2FoldChange lfcSE stat pvalue padj <numeric> <numeric> <numeric> <numeric> <numeric> <numeric> 1 66.1249 0.281757 0.727668 0.387206 0.698604 0.989804 2 76.9682 0.305763 0.619209 0.493796 0.621451 0.989804 3 64.7843 -0.694525 0.479445 -1.448603 0.147448 0.931561 4 123.0252 0.631247 0.688564 0.916758 0.359269 0.931561 5 93.2002 -0.453430 0.686043 -0.660936 0.508653 0.941951 ... ... ... ... ... ... ... 96 64.0177 0.757585137 0.682683 1.109718054 0.267121 0.931561 97 114.3689 -0.580010850 0.640313 -0.905823841 0.365029 0.931561 98 79.9620 0.000100617 0.612442 0.000164288 0.999869 0.999869 99 92.6614 0.563514308 0.716109 0.786910869 0.431334 0.939106 100 96.4410 -0.155268696 0.534400 -0.290547708 0.771397 0.989804 From the script you can see it has DESeq2 and optparse as dependencies. If we want to run the script inside a container, we would have to install them. We do this in the Dockerfile below. We give it the following instructions: use the r2u base image version jammy install the package DESeq2 , optparse and some additional packages we will need later on. We perform the installations with install2.r , which is a helper command that is present inside most rocker images. More info here . copy the script test_deseq2.R to /opt inside the container: FROM rocker/r2u:jammy RUN install2.r \\ DESeq2 \\ optparse \\ apeglm \\ IHW \\ limma \\ data.table \\ ggrepel \\ pheatmap \\ stringr COPY test_deseq2.R /opt Note In order to use COPY , the file that needs to be copied needs to be in the same directory as the Dockerfile or one of its subdirectories. R image stack The most used R image stack is from the rocker project . It contains many different base images (e.g. with shiny, Rstudio, tidyverse etc.). It depends on the type of image whether installations with apt-get or install2.r are possible. To understand more about how to install R packages in different containers, check it this cheat sheet , or visit rocker-project.org . Exercise: Download the test_deseq2.R and build the image with docker build . Name the image deseq2 . After that, start an interactive session and execute the script inside the container. Hint Make an interactive session with the options -i and -t and use /bin/bash as the command. Answer Build the container: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t deseq2 . docker build --platform amd64 -t deseq2 . Run the container: docker run -it --rm deseq2 /bin/bash Inside the container we look up the script: cd /opt ls This should return test_deseq2.R . Now you can execute it from inside the container: ./test_deseq2.R --rows 100 That\u2019s kind of nice. We can ship our R script inside our container. However, we don\u2019t want to run it interactively every time. So let\u2019s make some changes to make it easy to run it as an executable. For example, we can add /opt to the global $PATH variable with ENV . The $PATH variable The path variable is a special variable that consists of a list of path seperated by colons ( : ). These paths are searched if you are trying to run an executable. More info this topic at e.g. wikipedia . FROM rocker/r2u:jammy RUN install2.r \\ DESeq2 \\ optparse \\ apeglm \\ IHW \\ limma \\ data.table \\ ggrepel \\ pheatmap \\ stringr COPY test_deseq2.R /opt ENV PATH = /opt: $PATH Note The ENV instruction can be used to set any variable. Exercise : Rebuild the image and start an interactive bash session inside the new image. Is the path variable updated? (i.e. can we execute test_deseq2.R from anywhere?) Answer After re-building we start an interactive session: docker run -it --rm deseq2 /bin/bash The path is upated, /opt is appended to the beginning of the variable: echo $PATH returns: /opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin Now you can try to execute it from the root directory (or any other): test_deseq2.R Instead of starting an interactive session with /bin/bash we can now more easily run the script non-interactively: docker run --rm deseq2 test_deseq2.R --rows 100 Now it will directly print the output of test_deseq2.R to stdout. In the case you want to pack your script inside a container, you are building a container specifically for your script, meaning you almost want the container to behave as the program itself. In order to do that, you can use ENTRYPOINT . ENTRYPOINT is similar to CMD , but has two important differences: ENTRYPOINT can not be overwritten by the positional arguments (i.e. docker run image [CMD] ), but has to be overwritten by --entrypoint . The positional arguments (or CMD ) are pasted to the ENTRYPOINT command. This means that you can use ENTRYPOINT as the executable and the positional arguments (or CMD ) as the options. Let\u2019s try it out: FROM rocker/r2u:jammy RUN install2.r \\ DESeq2 \\ optparse \\ apeglm \\ IHW \\ limma \\ data.table \\ ggrepel \\ pheatmap \\ stringr COPY test_deseq2.R /opt ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"test_deseq2.R\" ] # default option (if positional arguments are not specified) CMD [ \"--rows\" , \"100\" ] Exercise : Re-build, and run the container non-interactively without any positional arguments. After that, try to pass a different number of rows to --rows . How do the commands look? Answer Just running the container non-interactively would be: docker run --rm deseq2 Passing a different argument (i.e. overwriting CMD ) would be: docker run --rm deseq2 --rows 200 Here, the container behaves as the executable itself to which you can pass arguments. Most containerized applications need multiple build steps. Often, you want to perform these steps and executions in a specific directory. Therefore, it can be in convenient to specify a working directory. You can do that with WORKDIR . This instruction will set the default directory for all other instructions (like RUN , COPY etc.). It will also change the directory in which you will land if you run the container interactively. FROM rocker/r2u:jammy RUN install2.r \\ DESeq2 \\ optparse \\ apeglm \\ IHW \\ limma \\ data.table \\ ggrepel \\ pheatmap \\ stringr WORKDIR /opt COPY test_deseq2.R . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"test_deseq2.R\" ] # default option (if positional arguments are not specified) CMD [ \"--rows\" , \"100\" ] Exercise : build the image, and start the container interactively. Has the default directory changed? After that, push the image to dockerhub, so we can use it later with the singularity exercises. Note You can overwrite ENTRYPOINT with --entrypoint as an argument to docker run . Answer Running the container interactively would be: docker run -it --rm --entrypoint /bin/bash deseq2 Which should result in a terminal looking something like this: root@9a27da455fb1:/opt# Meaning that indeed the default directory has changed to /opt Pushing it to dockerhub: docker tag deseq2 [ USER NAME ] /deseq2:v1 docker push [ USER NAME ] /deseq2:v1 Get information on your image with docker inspect We have used docker inspect already in the previous chapter to find the default Cmd of the ubuntu image. However we can get more info on the image: e.g. the entrypoint, environmental variables, cmd, workingdir etc., you can use the Config record from the output of docker inspect . For our image this looks like: \"Config\" : { \"Hostname\" : \"\" , \"Domainname\" : \"\" , \"User\" : \"\" , \"AttachStdin\" : false , \"AttachStdout\" : false , \"AttachStderr\" : false , \"Tty\" : false , \"OpenStdin\" : false , \"StdinOnce\" : false , \"Env\" : [ \"PATH=/opt:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" , \"LC_ALL=en_US.UTF-8\" , \"LANG=en_US.UTF-8\" , \"DEBIAN_FRONTEND=noninteractive\" , \"TZ=UTC\" ], \"Cmd\" : [ \"--rows\" , \"100\" ], \"ArgsEscaped\" : true , \"Image\" : \"\" , \"Volumes\" : null , \"WorkingDir\" : \"/opt\" , \"Entrypoint\" : [ \"test_deseq2.R\" ], \"OnBuild\" : null , \"Labels\" : { \"maintainer\" : \"Dirk Eddelbuettel <edd@debian.org>\" , \"org.label-schema.license\" : \"GPL-2.0\" , \"org.label-schema.vcs-url\" : \"https://github.com/rocker-org/\" , \"org.label-schema.vendor\" : \"Rocker Project\" } } Adding metadata to your image You can annotate your Dockerfile and the image by using the instruction LABEL . You can give it any key and value with <key>=<value> . However, it is recommended to use the Open Container Initiative (OCI) keys . Exercise : Annotate our Dockerfile with the OCI keys on the creation date, author and description. After that, check whether this has been passed to the actual image with docker inspect . Note You can type LABEL for each key-value pair, but you can also have it on one line by seperating the key-value pairs by a space, e.g.: LABEL keyx = \"valuex\" keyy = \"valuey\" Answer The Dockerfile would look like: FROM rocker/r2u:jammy LABEL org.opencontainers.image.created = \"2023-04-12\" \\ org.opencontainers.image.authors = \"Geert van Geest\" \\ org.opencontainers.image.description = \"Container with DESeq2 and friends\" RUN install2.r \\ DESeq2 \\ optparse \\ apeglm \\ IHW \\ limma \\ data.table \\ ggrepel \\ pheatmap \\ stringr WORKDIR /opt COPY test_deseq2.R . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"test_deseq2.R\" ] # default option (if positional arguments are not specified) CMD [ \"--rows\" , \"100\" ] The Config record in the output of docker inspect was updated with: \"Labels\" : { \"org.opencontainers.image.authors\" : \"Geert van Geest\" , \"org.opencontainers.image.created\" : \"2023-04-12\" , \"org.opencontainers.image.description\" : \"Container with DESeq2 and friends\" , \"org.opencontainers.image.licenses\" : \"GPL-2.0-or-later\" , \"org.opencontainers.image.source\" : \"https://github.com/rocker-org/rocker\" , \"org.opencontainers.image.vendor\" : \"Rocker Project\" } Building an image with a browser interface In this exercise, we will use a different base image ( rocker/rstudio:4 ), and we\u2019ll install the same packages. Rstudio server is a nice browser interface that you can use for a.o. programming in R. With the image we are creating we will be able to run Rstudio server inside a container. Check out the Dockerfile : FROM rocker/rstudio:4 RUN apt-get update && \\ apt-get install -y libz-dev RUN install2.r \\ optparse \\ BiocManager RUN R -q -e 'BiocManager::install(\"biomaRt\")' This will create an image from the existing rstudio image. It will also install libz-dev with apt-get , BiocManager with install2.r and DESeq2 with an R command. Despite we\u2019re installing the same packages, the installation steps need to be different from the r-base image. This is because in the rocker/rstudio images R is installed from source, and therefore you can\u2019t install packages with apt-get . More information on how to install R packages in R containers in this cheat sheet , or visit rocker-project.org . Installation will take a while The installation of CRAN packages will go relatively quickly, because can use the binary packages supplied by Posit Public Package Manager . However, the installation of Bioconductor packages will take a while, because they need to be installed from source. If you don\u2019t have time, you can skip the DESeq2 installation by removing the last line of the Dockerfile . Exercise: Build an image based on this Dockerfile and give it a meaningful name. Answer x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t rstudio-server . docker build --platform amd64 -t rstudio-server . You can now run a container from the image. However, you will have to tell docker where to publish port 8787 from the docker container with -p [HOSTPORT:CONTAINERPORT] . We choose to publish it to the same port number: docker run --rm -it -p 8787 :8787 rstudio-server Networking More info on docker container networking here By running the above command, a container will be started exposing rstudio server at port 8787 at localhost. You can approach the instance of Rstudio server by typing localhost:8787 in your browser. You will be asked for a password. You can find this password in the terminal from which you have started the container. We can make this even more interesting by mounting a local directory to the container running the Rstudio image: docker run \\ -it \\ --rm \\ -p 8787 :8787 \\ --mount type = bind,source = /Users/myusername/working_dir,target = /home/rstudio/working_dir \\ rstudio-server By doing this you have a completely isolated and shareable R environment running Rstudio server, but with your local files available to it. Pretty neat right?","title":"Working with dockerfiles"},{"location":"course_material/dockerfiles/#learning-outcomes","text":"After having completed this chapter you will be able to: Build an image based on a dockerfile Use the basic dockerfile syntax Change the default command of an image and validate the change Map ports to a container to display interactive content through a browser","title":"Learning outcomes"},{"location":"course_material/dockerfiles/#material","text":"Official Dockerfile reference Ten simple rules for writing dockerfiles","title":"Material"},{"location":"course_material/dockerfiles/#exercises","text":"To make your images shareable and adjustable, it\u2019s good practice to work with a Dockerfile . This is a script with a set of instructions to build your image from an existing image.","title":"Exercises"},{"location":"course_material/dockerfiles/#basic-dockerfile","text":"You can generate an image from a Dockerfile using the command docker build . A Dockerfile has its own syntax for giving instructions. Luckily, they are rather simple. The script always contains a line starting with FROM that takes the image name from which the new image will be built. After that you usually want to run some commands to e.g. configure and/or install software. The instruction to run these commands during building starts with RUN . In our figlet example that would be: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet On writing reproducible Dockerfiles At the FROM statement in the above Dockerfile you see that we have added a specific tag to the image (i.e. jammy-20230308 ). We could also have written: FROM ubuntu RUN apt-get update RUN apt-get install figlet This will automatically pull the image with the tag latest . However, if the maintainer of the ubuntu images decides to tag another ubuntu version as latest , rebuilding with the above Dockerfile will not give you the same result. Therefore it\u2019s always good practice to add the (stable) tag to the image in a Dockerfile . More rules on making your Dockerfiles more reproducible here . Exercise: Create a file on your computer called Dockerfile , and paste the above instruction lines in that file. Make the directory containing the Dockerfile your current directory. Build a new image based on that Dockerfile with: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build . docker build --platform amd64 . If using an Apple M1 chip (newer Macs) If you are using a computer with an Apple M1 chip, you have the less common ARM system architecture, which can limit transferability of images to (more common) x86_64/AMD64 machines. When building images on a Mac with an M1 chip (especially if you have sharing in mind), it\u2019s best to specify the --platform amd64 flag. The argument of docker build The command docker build takes a directory as input (providing . means the current directory). This directory should contain the Dockerfile , but it can also contain more of the build context, e.g. (python, R, shell) scripts that are required to build the image. What has happened? What is the name of the build image? Answer A new image was created based on the Dockerfile . You can check it with: docker image ls , which gives something like: REPOSITORY TAG IMAGE ID CREATED SIZE <none> <none> 92c980b09aad 7 seconds ago 101MB ubuntu-figlet latest e08b999c7978 About an hour ago 101MB ubuntu latest f63181f19b2f 30 hours ago 72.9MB It has created an image without a name or tag. That\u2019s a bit inconvenient. Exercise: Build a new image with a specific name. You can do that with adding the option -t to docker build . Before that, remove the nameless image. Hint An image without a name is usually a \u201cdangling image\u201d. You can remove those with docker image prune . Answer Remove the nameless image with docker image prune . After that, rebuild an image with a name: x86_64 / AMD64 ARM (MacOS M1 chip) docker build -t ubuntu-figlet:v2 . docker build --platform amd64 -t ubuntu-figlet:v2 .","title":"Basic Dockerfile"},{"location":"course_material/dockerfiles/#using-cmd","text":"As you might remember the second positional argument of docker run is a command (i.e. docker run IMAGE [CMD] ). If you leave it empty, it uses the default command. You can change the default command in the Dockerfile with an instruction starting with CMD . For example: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD figlet My image works! Exercise: Build a new image based on the above Dockerfile . Can you validate the change using docker image inspect ? Can you overwrite this default with docker run ? Answer Copy the new line to your Dockerfile , and build the new image like this: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t ubuntu-figlet:v3 . docker build --platform amd64 -t ubuntu-figlet:v3 . The command docker inspect ubuntu-figlet:v3 will give: \"Cmd\": [ \"/bin/sh\", \"-c\", \"figlet My image works!\" ] So the default command ( /bin/bash ) has changed to figlet My image works! Running the image (with clean-up ( --rm )): docker run --rm ubuntu-figlet:v3 Will result in: __ __ _ _ _ | \\/ |_ _ (_)_ __ ___ __ _ __ _ ___ __ _____ _ __| | _____| | | |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| | | | | | |_| | | | | | | | | (_| | (_| | __/ \\ V V / (_) | | | <\\__ \\_| |_| |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___| \\_/\\_/ \\___/|_| |_|\\_\\___(_) |___/ |___/ And of course you can overwrite the default command: docker run --rm ubuntu-figlet:v3 figlet another text Resulting in: _ _ _ _ __ _ _ __ ___ | |_| |__ ___ _ __ | |_ _____ _| |_ / _` | '_ \\ / _ \\| __| '_ \\ / _ \\ '__| | __/ _ \\ \\/ / __| | (_| | | | | (_) | |_| | | | __/ | | || __/> <| |_ \\__,_|_| |_|\\___/ \\__|_| |_|\\___|_| \\__\\___/_/\\_\\\\__| Two flavours of CMD You have seen in the output of docker inspect that docker translates the command (i.e. figlet \"my image works!\" ) into this: [\"/bin/sh\", \"-c\", \"figlet 'My image works!'\"] . The notation we used in the Dockerfile is the shell notation while the notation with the square brackets ( [] ) is the exec-notation . You can use both notations in your Dockerfile . Altough the shell notation is more readable, the exec notation is directly used by the image, and therefore less ambiguous. A Dockerfile with shell notation: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD figlet My image works! A Dockerfile with exec notation: FROM ubuntu:jammy-20230308 RUN apt-get update RUN apt-get install figlet CMD [ \"/bin/sh\" , \"-c\" , \"figlet My image works!\" ] Exercise: Now push our created image (with a version tag) to docker hub. We will use it later for the singularity exercises . Answer docker tag ubuntu-figlet:v3 [ USER NAME ] /ubuntu-figlet:v3 docker push [ USER NAME ] /ubuntu-figlet:v3","title":"Using CMD"},{"location":"course_material/dockerfiles/#build-an-image-for-your-own-script","text":"Often containers are built for a specific purpose. For example, you can use a container to ship all dependencies together with your developed set of scripts/programs. For that you will need to add your scripts to the container. That is quite easily done with the instruction COPY . However, in order to make your container more user-friendly, there are several additional instructions that can come in useful. We will treat the most frequently used ones below. Depending on your preference, either choose R or Python below. In the exercises will use a script called test_deseq2.R . This script will: Load the DESeq2 and optparse packages Load some additional packages to test their installations. We will use those packages later on in the course. Create and parse an option called --rows with optparse Create a dummy count matrix Run DESeq2 on the dummy count matrix Print the results to stdout You can download it here , or copy-paste it: test_deseq2.R #!/usr/bin/env Rscript # load packages required for this script write ( \"Loading packages required for this script\" , stderr ()) suppressPackageStartupMessages ({ library ( DESeq2 ) library ( optparse ) }) # load dependency packages for testing installations write ( \"Loading dependency packages for testing installations\" , stderr ()) suppressPackageStartupMessages ({ library ( apeglm ) library ( IHW ) library ( limma ) library ( data.table ) library ( ggplot2 ) library ( ggrepel ) library ( pheatmap ) library ( RColorBrewer ) library ( scales ) library ( stringr ) }) # parse options with optparse option_list <- list ( make_option ( c ( \"--rows\" ), type = \"integer\" , help = \"Number of rows in dummy matrix [default = %default]\" , default = 100 ) ) opt_parser <- OptionParser ( option_list = option_list , description = \"Runs DESeq2 on dummy data\" ) opt <- parse_args ( opt_parser ) # create a random dummy count matrix cnts <- matrix ( rnbinom ( n = opt $ row * 10 , mu = 100 , size = 1 / 0.5 ), ncol = 10 ) cond <- factor ( rep ( 1 : 2 , each = 5 )) # object construction dds <- DESeqDataSetFromMatrix ( cnts , DataFrame ( cond ), ~ cond ) # standard analysis dds <- DESeq ( dds ) res <- results ( dds ) # print results to stdout print ( res ) After you have downloaded it, make sure to set the permissions to executable: chmod +x test_deseq2.R It is a relatively simple script that runs DESeq2 on a dummy dataset. An example for execution would be: ./test_deseq2.R --rows 100 Here, --rows is a optional arguments that specifies the number of rows generated in the input count matrix. When running the script, it will return a bunch of messages and at the end an overview of differential gene expression analysis results: baseMean log2FoldChange lfcSE stat pvalue padj <numeric> <numeric> <numeric> <numeric> <numeric> <numeric> 1 66.1249 0.281757 0.727668 0.387206 0.698604 0.989804 2 76.9682 0.305763 0.619209 0.493796 0.621451 0.989804 3 64.7843 -0.694525 0.479445 -1.448603 0.147448 0.931561 4 123.0252 0.631247 0.688564 0.916758 0.359269 0.931561 5 93.2002 -0.453430 0.686043 -0.660936 0.508653 0.941951 ... ... ... ... ... ... ... 96 64.0177 0.757585137 0.682683 1.109718054 0.267121 0.931561 97 114.3689 -0.580010850 0.640313 -0.905823841 0.365029 0.931561 98 79.9620 0.000100617 0.612442 0.000164288 0.999869 0.999869 99 92.6614 0.563514308 0.716109 0.786910869 0.431334 0.939106 100 96.4410 -0.155268696 0.534400 -0.290547708 0.771397 0.989804 From the script you can see it has DESeq2 and optparse as dependencies. If we want to run the script inside a container, we would have to install them. We do this in the Dockerfile below. We give it the following instructions: use the r2u base image version jammy install the package DESeq2 , optparse and some additional packages we will need later on. We perform the installations with install2.r , which is a helper command that is present inside most rocker images. More info here . copy the script test_deseq2.R to /opt inside the container: FROM rocker/r2u:jammy RUN install2.r \\ DESeq2 \\ optparse \\ apeglm \\ IHW \\ limma \\ data.table \\ ggrepel \\ pheatmap \\ stringr COPY test_deseq2.R /opt Note In order to use COPY , the file that needs to be copied needs to be in the same directory as the Dockerfile or one of its subdirectories. R image stack The most used R image stack is from the rocker project . It contains many different base images (e.g. with shiny, Rstudio, tidyverse etc.). It depends on the type of image whether installations with apt-get or install2.r are possible. To understand more about how to install R packages in different containers, check it this cheat sheet , or visit rocker-project.org . Exercise: Download the test_deseq2.R and build the image with docker build . Name the image deseq2 . After that, start an interactive session and execute the script inside the container. Hint Make an interactive session with the options -i and -t and use /bin/bash as the command. Answer Build the container: x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t deseq2 . docker build --platform amd64 -t deseq2 . Run the container: docker run -it --rm deseq2 /bin/bash Inside the container we look up the script: cd /opt ls This should return test_deseq2.R . Now you can execute it from inside the container: ./test_deseq2.R --rows 100 That\u2019s kind of nice. We can ship our R script inside our container. However, we don\u2019t want to run it interactively every time. So let\u2019s make some changes to make it easy to run it as an executable. For example, we can add /opt to the global $PATH variable with ENV . The $PATH variable The path variable is a special variable that consists of a list of path seperated by colons ( : ). These paths are searched if you are trying to run an executable. More info this topic at e.g. wikipedia . FROM rocker/r2u:jammy RUN install2.r \\ DESeq2 \\ optparse \\ apeglm \\ IHW \\ limma \\ data.table \\ ggrepel \\ pheatmap \\ stringr COPY test_deseq2.R /opt ENV PATH = /opt: $PATH Note The ENV instruction can be used to set any variable. Exercise : Rebuild the image and start an interactive bash session inside the new image. Is the path variable updated? (i.e. can we execute test_deseq2.R from anywhere?) Answer After re-building we start an interactive session: docker run -it --rm deseq2 /bin/bash The path is upated, /opt is appended to the beginning of the variable: echo $PATH returns: /opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin Now you can try to execute it from the root directory (or any other): test_deseq2.R Instead of starting an interactive session with /bin/bash we can now more easily run the script non-interactively: docker run --rm deseq2 test_deseq2.R --rows 100 Now it will directly print the output of test_deseq2.R to stdout. In the case you want to pack your script inside a container, you are building a container specifically for your script, meaning you almost want the container to behave as the program itself. In order to do that, you can use ENTRYPOINT . ENTRYPOINT is similar to CMD , but has two important differences: ENTRYPOINT can not be overwritten by the positional arguments (i.e. docker run image [CMD] ), but has to be overwritten by --entrypoint . The positional arguments (or CMD ) are pasted to the ENTRYPOINT command. This means that you can use ENTRYPOINT as the executable and the positional arguments (or CMD ) as the options. Let\u2019s try it out: FROM rocker/r2u:jammy RUN install2.r \\ DESeq2 \\ optparse \\ apeglm \\ IHW \\ limma \\ data.table \\ ggrepel \\ pheatmap \\ stringr COPY test_deseq2.R /opt ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"test_deseq2.R\" ] # default option (if positional arguments are not specified) CMD [ \"--rows\" , \"100\" ] Exercise : Re-build, and run the container non-interactively without any positional arguments. After that, try to pass a different number of rows to --rows . How do the commands look? Answer Just running the container non-interactively would be: docker run --rm deseq2 Passing a different argument (i.e. overwriting CMD ) would be: docker run --rm deseq2 --rows 200 Here, the container behaves as the executable itself to which you can pass arguments. Most containerized applications need multiple build steps. Often, you want to perform these steps and executions in a specific directory. Therefore, it can be in convenient to specify a working directory. You can do that with WORKDIR . This instruction will set the default directory for all other instructions (like RUN , COPY etc.). It will also change the directory in which you will land if you run the container interactively. FROM rocker/r2u:jammy RUN install2.r \\ DESeq2 \\ optparse \\ apeglm \\ IHW \\ limma \\ data.table \\ ggrepel \\ pheatmap \\ stringr WORKDIR /opt COPY test_deseq2.R . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"test_deseq2.R\" ] # default option (if positional arguments are not specified) CMD [ \"--rows\" , \"100\" ] Exercise : build the image, and start the container interactively. Has the default directory changed? After that, push the image to dockerhub, so we can use it later with the singularity exercises. Note You can overwrite ENTRYPOINT with --entrypoint as an argument to docker run . Answer Running the container interactively would be: docker run -it --rm --entrypoint /bin/bash deseq2 Which should result in a terminal looking something like this: root@9a27da455fb1:/opt# Meaning that indeed the default directory has changed to /opt Pushing it to dockerhub: docker tag deseq2 [ USER NAME ] /deseq2:v1 docker push [ USER NAME ] /deseq2:v1","title":"Build an image for your own script"},{"location":"course_material/dockerfiles/#get-information-on-your-image-with-docker-inspect","text":"We have used docker inspect already in the previous chapter to find the default Cmd of the ubuntu image. However we can get more info on the image: e.g. the entrypoint, environmental variables, cmd, workingdir etc., you can use the Config record from the output of docker inspect . For our image this looks like: \"Config\" : { \"Hostname\" : \"\" , \"Domainname\" : \"\" , \"User\" : \"\" , \"AttachStdin\" : false , \"AttachStdout\" : false , \"AttachStderr\" : false , \"Tty\" : false , \"OpenStdin\" : false , \"StdinOnce\" : false , \"Env\" : [ \"PATH=/opt:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" , \"LC_ALL=en_US.UTF-8\" , \"LANG=en_US.UTF-8\" , \"DEBIAN_FRONTEND=noninteractive\" , \"TZ=UTC\" ], \"Cmd\" : [ \"--rows\" , \"100\" ], \"ArgsEscaped\" : true , \"Image\" : \"\" , \"Volumes\" : null , \"WorkingDir\" : \"/opt\" , \"Entrypoint\" : [ \"test_deseq2.R\" ], \"OnBuild\" : null , \"Labels\" : { \"maintainer\" : \"Dirk Eddelbuettel <edd@debian.org>\" , \"org.label-schema.license\" : \"GPL-2.0\" , \"org.label-schema.vcs-url\" : \"https://github.com/rocker-org/\" , \"org.label-schema.vendor\" : \"Rocker Project\" } }","title":"Get information on your image with docker inspect"},{"location":"course_material/dockerfiles/#adding-metadata-to-your-image","text":"You can annotate your Dockerfile and the image by using the instruction LABEL . You can give it any key and value with <key>=<value> . However, it is recommended to use the Open Container Initiative (OCI) keys . Exercise : Annotate our Dockerfile with the OCI keys on the creation date, author and description. After that, check whether this has been passed to the actual image with docker inspect . Note You can type LABEL for each key-value pair, but you can also have it on one line by seperating the key-value pairs by a space, e.g.: LABEL keyx = \"valuex\" keyy = \"valuey\" Answer The Dockerfile would look like: FROM rocker/r2u:jammy LABEL org.opencontainers.image.created = \"2023-04-12\" \\ org.opencontainers.image.authors = \"Geert van Geest\" \\ org.opencontainers.image.description = \"Container with DESeq2 and friends\" RUN install2.r \\ DESeq2 \\ optparse \\ apeglm \\ IHW \\ limma \\ data.table \\ ggrepel \\ pheatmap \\ stringr WORKDIR /opt COPY test_deseq2.R . ENV PATH = /opt: $PATH # note that if you want to be able to combine the two # both ENTRYPOINT and CMD need to written in the exec form ENTRYPOINT [ \"test_deseq2.R\" ] # default option (if positional arguments are not specified) CMD [ \"--rows\" , \"100\" ] The Config record in the output of docker inspect was updated with: \"Labels\" : { \"org.opencontainers.image.authors\" : \"Geert van Geest\" , \"org.opencontainers.image.created\" : \"2023-04-12\" , \"org.opencontainers.image.description\" : \"Container with DESeq2 and friends\" , \"org.opencontainers.image.licenses\" : \"GPL-2.0-or-later\" , \"org.opencontainers.image.source\" : \"https://github.com/rocker-org/rocker\" , \"org.opencontainers.image.vendor\" : \"Rocker Project\" }","title":"Adding metadata to your image"},{"location":"course_material/dockerfiles/#building-an-image-with-a-browser-interface","text":"In this exercise, we will use a different base image ( rocker/rstudio:4 ), and we\u2019ll install the same packages. Rstudio server is a nice browser interface that you can use for a.o. programming in R. With the image we are creating we will be able to run Rstudio server inside a container. Check out the Dockerfile : FROM rocker/rstudio:4 RUN apt-get update && \\ apt-get install -y libz-dev RUN install2.r \\ optparse \\ BiocManager RUN R -q -e 'BiocManager::install(\"biomaRt\")' This will create an image from the existing rstudio image. It will also install libz-dev with apt-get , BiocManager with install2.r and DESeq2 with an R command. Despite we\u2019re installing the same packages, the installation steps need to be different from the r-base image. This is because in the rocker/rstudio images R is installed from source, and therefore you can\u2019t install packages with apt-get . More information on how to install R packages in R containers in this cheat sheet , or visit rocker-project.org . Installation will take a while The installation of CRAN packages will go relatively quickly, because can use the binary packages supplied by Posit Public Package Manager . However, the installation of Bioconductor packages will take a while, because they need to be installed from source. If you don\u2019t have time, you can skip the DESeq2 installation by removing the last line of the Dockerfile . Exercise: Build an image based on this Dockerfile and give it a meaningful name. Answer x86_64 / AMD64 ARM64 (MacOS M1 chip) docker build -t rstudio-server . docker build --platform amd64 -t rstudio-server . You can now run a container from the image. However, you will have to tell docker where to publish port 8787 from the docker container with -p [HOSTPORT:CONTAINERPORT] . We choose to publish it to the same port number: docker run --rm -it -p 8787 :8787 rstudio-server Networking More info on docker container networking here By running the above command, a container will be started exposing rstudio server at port 8787 at localhost. You can approach the instance of Rstudio server by typing localhost:8787 in your browser. You will be asked for a password. You can find this password in the terminal from which you have started the container. We can make this even more interesting by mounting a local directory to the container running the Rstudio image: docker run \\ -it \\ --rm \\ -p 8787 :8787 \\ --mount type = bind,source = /Users/myusername/working_dir,target = /home/rstudio/working_dir \\ rstudio-server By doing this you have a completely isolated and shareable R environment running Rstudio server, but with your local files available to it. Pretty neat right?","title":"Building an image with a browser interface"},{"location":"course_material/introduction_containers/","text":"Learning outcomes After having completed this chapter you will be able to: Discriminate between an image and a container Run a docker container from dockerhub interactively Validate the available containers and their status Material General introduction: Download the presentation Introduction to containers: Download the presentation Exercises If working on Windows If you are working on Windows, you can either use WSL2 or the local terminal in MobaXterm . Make sure you install the latest versions before you install docker. In principle, you can also use a native shell like PowerShell, but this might result into some issues with bind mounting directories. Let\u2019s create our first container from an existing image. We do this with the image ubuntu , generating an environment with a minimal installation of ubuntu. docker run -it ubuntu This will give you an interactive shell into the created container (this interactivity was invoked by the options -i and -t ) . Exercise: Check out the operating system of the container by typing cat /etc/os-release in the container\u2019s shell. Are we really in an ubuntu environment? Answer Yes: root@27f7d11608de:/# cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"20.04.1 LTS (Focal Fossa)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 20.04.1 LTS\" VERSION_ID=\"20.04\" HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME=focal UBUNTU_CODENAME=focal Where does the image come from? If the image ubuntu was not on your computer yet, docker will search and try to get them from dockerhub , and download it. Exercise: Run the command whoami in the docker container. Who are you? Answer The command whoami returns the current user. In the container whoami will return root . This means you are the root user i.e. within the container you are admin and can basically change anything. Check out the container panel at the Docker dashboard (the Docker gui) or open another host terminal and type: docker container ls -a Exercise: What is the container status? Answer In Docker dashboard you can see that the shell is running: The output of docker container ls -a is: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 27f7d11608de ubuntu \"/bin/bash\" 7 minutes ago Up 6 minutes great_moser Also showing you that the STATUS is Up . Now let\u2019s install some software in our ubuntu environment. We\u2019ll install some simple software called figlet . Type into the container shell: apt-get update apt-get install figlet This will give some warnings This installation will give some warnings. It\u2019s safe to ignore them. Now let\u2019s try it out. Type into the container shell: figlet 'SIB courses are great!' Now you have installed and used software figlet in an ubuntu environment (almost) completely separated from your host computer. This already gives you an idea of the power of containerization. Exit the shell by typing exit . Check out the container panel of Docker dashboard or type: docker container ls -a Exercise: What is the container status? Answer docker container ls -a gives: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 27f7d11608de ubuntu \"/bin/bash\" 15 minutes ago Exited (0) 8 seconds ago great_moser Showing that the container has exited, meaning it\u2019s not running.","title":"Introduction to containers"},{"location":"course_material/introduction_containers/#learning-outcomes","text":"After having completed this chapter you will be able to: Discriminate between an image and a container Run a docker container from dockerhub interactively Validate the available containers and their status","title":"Learning outcomes"},{"location":"course_material/introduction_containers/#material","text":"General introduction: Download the presentation Introduction to containers: Download the presentation","title":"Material"},{"location":"course_material/introduction_containers/#exercises","text":"If working on Windows If you are working on Windows, you can either use WSL2 or the local terminal in MobaXterm . Make sure you install the latest versions before you install docker. In principle, you can also use a native shell like PowerShell, but this might result into some issues with bind mounting directories. Let\u2019s create our first container from an existing image. We do this with the image ubuntu , generating an environment with a minimal installation of ubuntu. docker run -it ubuntu This will give you an interactive shell into the created container (this interactivity was invoked by the options -i and -t ) . Exercise: Check out the operating system of the container by typing cat /etc/os-release in the container\u2019s shell. Are we really in an ubuntu environment? Answer Yes: root@27f7d11608de:/# cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"20.04.1 LTS (Focal Fossa)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 20.04.1 LTS\" VERSION_ID=\"20.04\" HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME=focal UBUNTU_CODENAME=focal Where does the image come from? If the image ubuntu was not on your computer yet, docker will search and try to get them from dockerhub , and download it. Exercise: Run the command whoami in the docker container. Who are you? Answer The command whoami returns the current user. In the container whoami will return root . This means you are the root user i.e. within the container you are admin and can basically change anything. Check out the container panel at the Docker dashboard (the Docker gui) or open another host terminal and type: docker container ls -a Exercise: What is the container status? Answer In Docker dashboard you can see that the shell is running: The output of docker container ls -a is: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 27f7d11608de ubuntu \"/bin/bash\" 7 minutes ago Up 6 minutes great_moser Also showing you that the STATUS is Up . Now let\u2019s install some software in our ubuntu environment. We\u2019ll install some simple software called figlet . Type into the container shell: apt-get update apt-get install figlet This will give some warnings This installation will give some warnings. It\u2019s safe to ignore them. Now let\u2019s try it out. Type into the container shell: figlet 'SIB courses are great!' Now you have installed and used software figlet in an ubuntu environment (almost) completely separated from your host computer. This already gives you an idea of the power of containerization. Exit the shell by typing exit . Check out the container panel of Docker dashboard or type: docker container ls -a Exercise: What is the container status? Answer docker container ls -a gives: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 27f7d11608de ubuntu \"/bin/bash\" 15 minutes ago Exited (0) 8 seconds ago great_moser Showing that the container has exited, meaning it\u2019s not running.","title":"Exercises"},{"location":"course_material/managing_docker/","text":"Learning outcomes After having completed this chapter you will be able to: Explain the concept of layers in the context of docker containers and images Use the command line to restart and re-attach to an exited container Create a new image with docker commit List locally available images with docker image ls Run a command inside a container non-interactively Use docker image inspect to get more information on an image Use the command line to prune dangling images and stopped containers Rename and tag a docker image Push a newly created image to dockerhub Use the option --mount to bind mount a host directory to a container Material Download the presentation Overview of how docker works More on bind mounts Docker volumes in general Exercises Restarting an exited container If you would like to go back to your container with the figlet installation, you could try to run again: docker run -it ubuntu Exercise: Run the above command. Is your figlet installation still there? Why? Hint Check the status of your containers: docker container ls -a Answer No, the installation is gone. Another container was created from the same ubuntu image, without the figlet installation. Running the command docker container ls -a results in: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8d7c4c611b70 ubuntu \"/bin/bash\" About a minute ago Up About a minute kind_mendel 27f7d11608de ubuntu \"/bin/bash\" 27 minutes ago Exited (0) 2 minutes ago great_moser In this case the container great_moser contains the figlet installation. But we have exited that container. We created a new container ( kind_mendel in this case) with a fresh environment created from the original ubuntu image. To restart your first created container, you\u2019ll have to look up its name. You can find it in the Docker dashboard, or with docker container ls -a . Container names The container name is the funny combination of two words separated by _ , e.g.: nifty_sinoussi . Alternatively you can use the container ID (the first column of the output of docker container ls ) To restart a container you can use: docker start [ CONTAINER NAME ] And after that to re-attach to the shell: docker attach [ CONTAINER NAME ] And you\u2019re back in the container shell. Exercise: Run the docker start and docker attach commands for the container that is supposed to contain the figlet installation. Is the installation of figlet still there? Answer yes: figlet 'try some more text!' Should give you output. docker attach and docker exec In addition to docker attach , you can also \u201cre-attach\u201d a container with docker exec . However, these two are quite different. While docker attach gets you back to your stopped shell process, docker exec creates a new one (more information on stackoverflow ). The command docker exec enables you therefore to have multiple shells open in the same container. That can be convenient if you have one shell open with a program running in the foreground, and another one for e.g. monitoring. An example for using docker exec on a running container: docker exec -it [ CONTAINER NAME ] /bin/bash Note that docker exec requires a CMD, it doesn\u2019t use the default. Creating a new image You can store your changes and create a new image based on the ubuntu image like this: docker commit [ CONTAINER NAME ] ubuntu-figlet Exercise: Run the above command with the name of the container containing the figlet installation. Check out docker image ls . What have we just created? Answer A new image called ubuntu-figlet based on the status of the container. The output of docker image ls should look like: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu-figlet latest e08b999c7978 4 seconds ago 101MB ubuntu latest f63181f19b2f 29 hours ago 72.9MB Now you can generate a new container based on the new image: docker run -it ubuntu-figlet Exercise: Run the above command. Is the figlet installation in the created container? Answer yes Commands The second positional argument of docker run can be a command followed by its arguments. So, we could run a container non-interactively (without -it ), and just let it run a single command: docker run ubuntu-figlet figlet 'non-interactive run' Resulting in just the output of the figlet command. In the previous exercises we have run containers without a command as positional argument. This doesn\u2019t mean that no command has been run, because the container would do nothing without a command. The default command is stored in the image, and you can find it by docker image inspect [IMAGE NAME] . Exercise: Have a look at the output of docker image inspect , particularly at \"Config\" (ignore \"ContainerConfig\" for now). What is the default command ( CMD ) of the ubuntu image? Answer Running docker image inspect ubuntu gives (amongst other information): \"Cmd\" : [ \"/bin/bash\" ] , In the case of the ubuntu the default command is bash , returning a shell in bash (i.e. Bourne again shell ). Adding the options -i and -t ( -it ) to your docker run command will therefore result in an interactive bash shell. You can modify this default behaviour. More on that later, when we will work on Dockerfiles . The difference between Config and ContainerConfig The configuration at Config represents the image, the configuration at ContainerConfig the last step during the build of the image, i.e. the last layer. More info e.g. at this post at stackoverflow . Removing containers In the meantime, with every call of docker run we have created a new container (check your containers with docker container ls -a ). You probably don\u2019t want to remove those one-by-one. These two commands are very useful to clean up your Docker cache: docker container prune : removes stopped containers docker image prune : removes dangling images (i.e. images without a name) So, remove your stopped containers with: docker container prune Unless you\u2019re developing further on a container, or you\u2019re using it for an analysis, you probably want to get rid of it once you have exited the container. You can do this with adding --rm to your docker run command, e.g.: docker run --rm ubuntu-figlet figlet 'non-interactive run' Pushing to dockerhub Now that we have created our first own docker image, we can store it and share it with the world on docker hub. Before we get there, we first have to (re)name and tag it. Before pushing an image to dockerhub, docker has to know to which user and which repository the image should be added. That information should be in the name of the image, like this: user/imagename . We can rename an image with docker tag (which is a bit of misleading name for the command). So we could push to dockerhub like this: docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet docker push [USER NAME]/ubuntu-figlet If on Linux If you are on Linux and haven\u2019t connected to docker hub before, you will have login first. To do that, run: docker login How docker makes money All images pushed to dockerhub are open to the world. With a free account you can have one image on dockerhub that is private. Paid accounts can have more private images, and are therefore popular for commercial organisations. As an alternative to dockerhub, you can store images locally with docker save . We didn\u2019t specify the tag for our new image. That\u2019s why docker tag gave it the default tag called latest . Pushing an image without a tag will overwrite the current image with the tag latest (more on (not) using latest here ). If you want to maintain multiple versions of your image, you will have to add a tag, and push the image with that tag to dockerhub: docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet:v1 docker push [USER NAME]/ubuntu-figlet:v1 Mounting a directory For many analyses you do calculations with files or scripts that are on your host (local) computer. But how do you make them available to a docker container? You can do that in several ways, but here we will use bind-mount. You can bind-mount a directory with -v ( --volume ) or --mount . Most old-school docker users will use -v , but --mount syntax is easier to understand and now recommended, so we will use the latter here: docker run \\ --mount type = bind,source = /host/source/path,target = /path/in/container \\ [ IMAGE ] The target directory will be created if it does not yet exist. The source directory should exist. MobaXterm users You can specify your local path with the Windows syntax (e.g. C:\\Users\\myusername ). However, you will have to use forward slashes ( / ) instead of backward slashes ( \\ ). Therefore, mounting a directory would look like: docker run \\ --mount type = bind,source = C:/Users/myusername,target = /path/in/container \\ [ IMAGE ] Do not use autocompletion or variable substitution (e.g. $PWD ) in MobaXterm, since these point to \u2018emulated\u2019 paths, and are not passed properly to the docker command. Using docker from Windows PowerShell Most of the syntax for docker is the same for both PowerShell and UNIX-based systems. However, there are some differences, e.g. in Windows, directories in file paths are separated by \\ instead of / . Also, line breaks are not escaped by \\ but by `. Exercise: Mount a host (local) directory to a target directory /working_dir in a container created from the ubuntu-figlet image and run it interactively. Check whether the target directory has been created. Answer e.g. on Mac OS this would be: docker run \\ -it \\ --mount type = bind,source = /Users/myusername/working_dir,target = /working_dir/ \\ ubuntu-figlet This creates a directory called working_dir in the root directory ( / ): root@8d80a8698865:/# ls bin dev home lib32 libx32 mnt proc run srv tmp var boot etc lib lib64 media opt root sbin sys usr working_dir This mounted directory is both available for the host (locally) and for the container. You can therefore e.g. copy files in there, and write output generated by the container. Exercise: Write the output of figlet \"testing mounted dir\" to a file in /working_dir . Check whether it is available on the host (locally) in the source directory. Hint You can write the output of figlet to a file like this: figlet 'some string' > file.txt Answer root@8d80a8698865:/# figlet 'testing mounted dir' > /working_dir/figlet_output.txt This should create a file in both your host (local) source directory and the target directory in the container called figlet_output.txt . Using files on the host This of course also works the other way around. If you would have a file on the host with e.g. a text, you can copy it into your mounted directory, and it will be available to the container. Managing permissions (extra) Depending on your system, the user ID and group ID will be taken over from the user inside the container. If the user inside the container is root, this will be root. That\u2019s a bit inconvenient if you just want to run the container as a regular user (for example in certain circumstances your container could write in / ). To do that, use the -u option, and specify the group ID and user ID like this: docker run -u [ uid ] : [ gid ] So, e.g.: docker run \\ -it \\ -u 1000 :1000 \\ --mount type = bind,source = /Users/myusername/working_dir,target = /working_dir/ \\ ubuntu-figlet If you want docker to take over your current uid and gid, you can use: docker run -u \"$(id -u):$(id -g)\" This behaviour is different on MacOS and MobaXterm On MacOS and in the local shell of MobaXterm the uid and gid are taken over from the user running the container (even if you set -u as 0:0), i.e. your current ID. More info on stackoverflow . Exercise: Start an interactive container based on the ubuntu-figlet image, bind-mount a local directory and take over your current uid and gid . Write the output of a figlet command to a file in the mounted directory. Who and which group owns the file inside the container? And outside the container? Answer the same question but now run the container without setting -u . Answer Linux MacOS MobaXterm Running ubuntu-figlet interactively while taking over uid and gid and mounting my current directory: docker run -it --mount type = bind,source = $PWD ,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=1000 gid=1000 groups=1000 So, I have taken over uid 1000 and gid 1000. I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 1000 1000 0 Mar 400 13:37 uid_set.txt So the file belongs to user 1000, and group 1000. Outside container: ubuntu@ip-172-31-33-21:~$ ls -lh -rw-r--r-- 1 ubuntu ubuntu 400 Mar 5 13:37 uid_set.txt Which makes sense: ubuntu@ip-172-31-33-21:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu) Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = $PWD ,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, uid and gid are root . root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 1000 1000 400 Mar 5 13:37 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:40 uid_unset.txt Outside container: ubuntu@ip-172-31-33-21:~$ ls -lh -rw-r--r-- 1 ubuntu ubuntu 0 Mar 5 13:37 uid_set.txt -rw-r--r-- 1 root root 0 Mar 5 13:40 uid_unset.txt So, the uid and gid 0 (root:root) are taken over. Running ubuntu-figlet interactively while taking over uid and gid and mounting my current directory: docker run -it --mount type = bind,source = $PWD ,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=503 gid=20(dialout) groups=20(dialout) So, the container has taken over uid 503 and group 20 I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 503 dialout 400 Mar 5 13:11 uid_set.txt So the file belongs to user 503, and the group dialout . Outside container: mac-34392:~ geertvangeest$ ls -lh -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:11 uid_set.txt Which are the same as inside the container: mac-34392:~ geertvangeest$ echo \"$(id -u):$(id -g)\" 503:20 The uid 503 was nameless in the docker container. However the group 20 already existed in the ubuntu container, and was named dialout . Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = $PWD ,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, inside the container I am root . Creating new files will lead to ownership of root inside the container: root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 503 dialout 400 Mar 5 13:11 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:25 uid_unset.txt Outside container: mac-34392:~ geertvangeest$ ls -lh -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:11 uid_set.txt -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:15 uid_unset.txt So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used. Running ubuntu-figlet interactively while taking over uid and gid and mounting to a specfied directory: docker run -it --mount type = bind,source = C:/Users/geert/data,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=1003 gid=513 groups=513 So, the container has taken over uid 1003 and group 513 I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 1003 513 400 Mar 5 13:11 uid_set.txt So the file belongs to user 1003, and the group 513. Outside container: /home/mobaxterm/data$ ls -lh -rwx------ 1 geert UserGrp 400 Mar 5 14:11 uid_set.txt Which are the same as inside the container: /home/mobaxterm/data$ echo \"$(id -u):$(id -g)\" 1003:513 Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = C:/Users/geert/data,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, inside the container I am root . Creating new files will lead to ownership of root inside the container: root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 1003 503 400 Mar 5 13:11 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:25 uid_unset.txt Outside container: /home/mobaxterm/data$ ls -lh -rwx------ 1 geert UserGrp 400 Mar 5 14:11 uid_set.txt -rwx------ 1 geert UserGrp 400 Mar 5 14:15 uid_unset.txt So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used.","title":"Managing containers and images"},{"location":"course_material/managing_docker/#learning-outcomes","text":"After having completed this chapter you will be able to: Explain the concept of layers in the context of docker containers and images Use the command line to restart and re-attach to an exited container Create a new image with docker commit List locally available images with docker image ls Run a command inside a container non-interactively Use docker image inspect to get more information on an image Use the command line to prune dangling images and stopped containers Rename and tag a docker image Push a newly created image to dockerhub Use the option --mount to bind mount a host directory to a container","title":"Learning outcomes"},{"location":"course_material/managing_docker/#material","text":"Download the presentation Overview of how docker works More on bind mounts Docker volumes in general","title":"Material"},{"location":"course_material/managing_docker/#exercises","text":"","title":"Exercises"},{"location":"course_material/managing_docker/#restarting-an-exited-container","text":"If you would like to go back to your container with the figlet installation, you could try to run again: docker run -it ubuntu Exercise: Run the above command. Is your figlet installation still there? Why? Hint Check the status of your containers: docker container ls -a Answer No, the installation is gone. Another container was created from the same ubuntu image, without the figlet installation. Running the command docker container ls -a results in: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8d7c4c611b70 ubuntu \"/bin/bash\" About a minute ago Up About a minute kind_mendel 27f7d11608de ubuntu \"/bin/bash\" 27 minutes ago Exited (0) 2 minutes ago great_moser In this case the container great_moser contains the figlet installation. But we have exited that container. We created a new container ( kind_mendel in this case) with a fresh environment created from the original ubuntu image. To restart your first created container, you\u2019ll have to look up its name. You can find it in the Docker dashboard, or with docker container ls -a . Container names The container name is the funny combination of two words separated by _ , e.g.: nifty_sinoussi . Alternatively you can use the container ID (the first column of the output of docker container ls ) To restart a container you can use: docker start [ CONTAINER NAME ] And after that to re-attach to the shell: docker attach [ CONTAINER NAME ] And you\u2019re back in the container shell. Exercise: Run the docker start and docker attach commands for the container that is supposed to contain the figlet installation. Is the installation of figlet still there? Answer yes: figlet 'try some more text!' Should give you output. docker attach and docker exec In addition to docker attach , you can also \u201cre-attach\u201d a container with docker exec . However, these two are quite different. While docker attach gets you back to your stopped shell process, docker exec creates a new one (more information on stackoverflow ). The command docker exec enables you therefore to have multiple shells open in the same container. That can be convenient if you have one shell open with a program running in the foreground, and another one for e.g. monitoring. An example for using docker exec on a running container: docker exec -it [ CONTAINER NAME ] /bin/bash Note that docker exec requires a CMD, it doesn\u2019t use the default.","title":"Restarting an exited container"},{"location":"course_material/managing_docker/#creating-a-new-image","text":"You can store your changes and create a new image based on the ubuntu image like this: docker commit [ CONTAINER NAME ] ubuntu-figlet Exercise: Run the above command with the name of the container containing the figlet installation. Check out docker image ls . What have we just created? Answer A new image called ubuntu-figlet based on the status of the container. The output of docker image ls should look like: REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu-figlet latest e08b999c7978 4 seconds ago 101MB ubuntu latest f63181f19b2f 29 hours ago 72.9MB Now you can generate a new container based on the new image: docker run -it ubuntu-figlet Exercise: Run the above command. Is the figlet installation in the created container? Answer yes","title":"Creating a new image"},{"location":"course_material/managing_docker/#commands","text":"The second positional argument of docker run can be a command followed by its arguments. So, we could run a container non-interactively (without -it ), and just let it run a single command: docker run ubuntu-figlet figlet 'non-interactive run' Resulting in just the output of the figlet command. In the previous exercises we have run containers without a command as positional argument. This doesn\u2019t mean that no command has been run, because the container would do nothing without a command. The default command is stored in the image, and you can find it by docker image inspect [IMAGE NAME] . Exercise: Have a look at the output of docker image inspect , particularly at \"Config\" (ignore \"ContainerConfig\" for now). What is the default command ( CMD ) of the ubuntu image? Answer Running docker image inspect ubuntu gives (amongst other information): \"Cmd\" : [ \"/bin/bash\" ] , In the case of the ubuntu the default command is bash , returning a shell in bash (i.e. Bourne again shell ). Adding the options -i and -t ( -it ) to your docker run command will therefore result in an interactive bash shell. You can modify this default behaviour. More on that later, when we will work on Dockerfiles . The difference between Config and ContainerConfig The configuration at Config represents the image, the configuration at ContainerConfig the last step during the build of the image, i.e. the last layer. More info e.g. at this post at stackoverflow .","title":"Commands"},{"location":"course_material/managing_docker/#removing-containers","text":"In the meantime, with every call of docker run we have created a new container (check your containers with docker container ls -a ). You probably don\u2019t want to remove those one-by-one. These two commands are very useful to clean up your Docker cache: docker container prune : removes stopped containers docker image prune : removes dangling images (i.e. images without a name) So, remove your stopped containers with: docker container prune Unless you\u2019re developing further on a container, or you\u2019re using it for an analysis, you probably want to get rid of it once you have exited the container. You can do this with adding --rm to your docker run command, e.g.: docker run --rm ubuntu-figlet figlet 'non-interactive run'","title":"Removing containers"},{"location":"course_material/managing_docker/#pushing-to-dockerhub","text":"Now that we have created our first own docker image, we can store it and share it with the world on docker hub. Before we get there, we first have to (re)name and tag it. Before pushing an image to dockerhub, docker has to know to which user and which repository the image should be added. That information should be in the name of the image, like this: user/imagename . We can rename an image with docker tag (which is a bit of misleading name for the command). So we could push to dockerhub like this: docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet docker push [USER NAME]/ubuntu-figlet If on Linux If you are on Linux and haven\u2019t connected to docker hub before, you will have login first. To do that, run: docker login How docker makes money All images pushed to dockerhub are open to the world. With a free account you can have one image on dockerhub that is private. Paid accounts can have more private images, and are therefore popular for commercial organisations. As an alternative to dockerhub, you can store images locally with docker save . We didn\u2019t specify the tag for our new image. That\u2019s why docker tag gave it the default tag called latest . Pushing an image without a tag will overwrite the current image with the tag latest (more on (not) using latest here ). If you want to maintain multiple versions of your image, you will have to add a tag, and push the image with that tag to dockerhub: docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet:v1 docker push [USER NAME]/ubuntu-figlet:v1","title":"Pushing to dockerhub"},{"location":"course_material/managing_docker/#mounting-a-directory","text":"For many analyses you do calculations with files or scripts that are on your host (local) computer. But how do you make them available to a docker container? You can do that in several ways, but here we will use bind-mount. You can bind-mount a directory with -v ( --volume ) or --mount . Most old-school docker users will use -v , but --mount syntax is easier to understand and now recommended, so we will use the latter here: docker run \\ --mount type = bind,source = /host/source/path,target = /path/in/container \\ [ IMAGE ] The target directory will be created if it does not yet exist. The source directory should exist. MobaXterm users You can specify your local path with the Windows syntax (e.g. C:\\Users\\myusername ). However, you will have to use forward slashes ( / ) instead of backward slashes ( \\ ). Therefore, mounting a directory would look like: docker run \\ --mount type = bind,source = C:/Users/myusername,target = /path/in/container \\ [ IMAGE ] Do not use autocompletion or variable substitution (e.g. $PWD ) in MobaXterm, since these point to \u2018emulated\u2019 paths, and are not passed properly to the docker command. Using docker from Windows PowerShell Most of the syntax for docker is the same for both PowerShell and UNIX-based systems. However, there are some differences, e.g. in Windows, directories in file paths are separated by \\ instead of / . Also, line breaks are not escaped by \\ but by `. Exercise: Mount a host (local) directory to a target directory /working_dir in a container created from the ubuntu-figlet image and run it interactively. Check whether the target directory has been created. Answer e.g. on Mac OS this would be: docker run \\ -it \\ --mount type = bind,source = /Users/myusername/working_dir,target = /working_dir/ \\ ubuntu-figlet This creates a directory called working_dir in the root directory ( / ): root@8d80a8698865:/# ls bin dev home lib32 libx32 mnt proc run srv tmp var boot etc lib lib64 media opt root sbin sys usr working_dir This mounted directory is both available for the host (locally) and for the container. You can therefore e.g. copy files in there, and write output generated by the container. Exercise: Write the output of figlet \"testing mounted dir\" to a file in /working_dir . Check whether it is available on the host (locally) in the source directory. Hint You can write the output of figlet to a file like this: figlet 'some string' > file.txt Answer root@8d80a8698865:/# figlet 'testing mounted dir' > /working_dir/figlet_output.txt This should create a file in both your host (local) source directory and the target directory in the container called figlet_output.txt . Using files on the host This of course also works the other way around. If you would have a file on the host with e.g. a text, you can copy it into your mounted directory, and it will be available to the container.","title":"Mounting a directory"},{"location":"course_material/managing_docker/#managing-permissions-extra","text":"Depending on your system, the user ID and group ID will be taken over from the user inside the container. If the user inside the container is root, this will be root. That\u2019s a bit inconvenient if you just want to run the container as a regular user (for example in certain circumstances your container could write in / ). To do that, use the -u option, and specify the group ID and user ID like this: docker run -u [ uid ] : [ gid ] So, e.g.: docker run \\ -it \\ -u 1000 :1000 \\ --mount type = bind,source = /Users/myusername/working_dir,target = /working_dir/ \\ ubuntu-figlet If you want docker to take over your current uid and gid, you can use: docker run -u \"$(id -u):$(id -g)\" This behaviour is different on MacOS and MobaXterm On MacOS and in the local shell of MobaXterm the uid and gid are taken over from the user running the container (even if you set -u as 0:0), i.e. your current ID. More info on stackoverflow . Exercise: Start an interactive container based on the ubuntu-figlet image, bind-mount a local directory and take over your current uid and gid . Write the output of a figlet command to a file in the mounted directory. Who and which group owns the file inside the container? And outside the container? Answer the same question but now run the container without setting -u . Answer Linux MacOS MobaXterm Running ubuntu-figlet interactively while taking over uid and gid and mounting my current directory: docker run -it --mount type = bind,source = $PWD ,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=1000 gid=1000 groups=1000 So, I have taken over uid 1000 and gid 1000. I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 1000 1000 0 Mar 400 13:37 uid_set.txt So the file belongs to user 1000, and group 1000. Outside container: ubuntu@ip-172-31-33-21:~$ ls -lh -rw-r--r-- 1 ubuntu ubuntu 400 Mar 5 13:37 uid_set.txt Which makes sense: ubuntu@ip-172-31-33-21:~$ id uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu) Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = $PWD ,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, uid and gid are root . root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 1000 1000 400 Mar 5 13:37 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:40 uid_unset.txt Outside container: ubuntu@ip-172-31-33-21:~$ ls -lh -rw-r--r-- 1 ubuntu ubuntu 0 Mar 5 13:37 uid_set.txt -rw-r--r-- 1 root root 0 Mar 5 13:40 uid_unset.txt So, the uid and gid 0 (root:root) are taken over. Running ubuntu-figlet interactively while taking over uid and gid and mounting my current directory: docker run -it --mount type = bind,source = $PWD ,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=503 gid=20(dialout) groups=20(dialout) So, the container has taken over uid 503 and group 20 I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 503 dialout 400 Mar 5 13:11 uid_set.txt So the file belongs to user 503, and the group dialout . Outside container: mac-34392:~ geertvangeest$ ls -lh -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:11 uid_set.txt Which are the same as inside the container: mac-34392:~ geertvangeest$ echo \"$(id -u):$(id -g)\" 503:20 The uid 503 was nameless in the docker container. However the group 20 already existed in the ubuntu container, and was named dialout . Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = $PWD ,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, inside the container I am root . Creating new files will lead to ownership of root inside the container: root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 503 dialout 400 Mar 5 13:11 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:25 uid_unset.txt Outside container: mac-34392:~ geertvangeest$ ls -lh -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:11 uid_set.txt -rw-r--r-- 1 geertvangeest staff 400B Mar 5 14:15 uid_unset.txt So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used. Running ubuntu-figlet interactively while taking over uid and gid and mounting to a specfied directory: docker run -it --mount type = bind,source = C:/Users/geert/data,target = /data -u \" $( id -u ) : $( id -g ) \" ubuntu-figlet Inside container: I have no name!@e808d7c36e7c:/$ id uid=1003 gid=513 groups=513 So, the container has taken over uid 1003 and group 513 I have no name!@e808d7c36e7c:/$ cd /data I have no name!@e808d7c36e7c:/data$ figlet 'uid set' > uid_set.txt I have no name!@e808d7c36e7c:/data$ ls -lh -rw-r--r-- 1 1003 513 400 Mar 5 13:11 uid_set.txt So the file belongs to user 1003, and the group 513. Outside container: /home/mobaxterm/data$ ls -lh -rwx------ 1 geert UserGrp 400 Mar 5 14:11 uid_set.txt Which are the same as inside the container: /home/mobaxterm/data$ echo \"$(id -u):$(id -g)\" 1003:513 Running ubuntu-figlet interactively without taking over uid and gid : docker run -it --mount type = bind,source = C:/Users/geert/data,target = /data ubuntu-figlet Inside container: root@fface8afb220:/# id uid=0(root) gid=0(root) groups=0(root) So, inside the container I am root . Creating new files will lead to ownership of root inside the container: root@fface8afb220:/# cd /data root@fface8afb220:/data# figlet 'uid unset' > uid_unset.txt root@fface8afb220:/data# ls -lh -rw-r--r-- 1 1003 503 400 Mar 5 13:11 uid_set.txt -rw-r--r-- 1 root root 400 Mar 5 13:25 uid_unset.txt Outside container: /home/mobaxterm/data$ ls -lh -rwx------ 1 geert UserGrp 400 Mar 5 14:11 uid_set.txt -rwx------ 1 geert UserGrp 400 Mar 5 14:15 uid_unset.txt So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used.","title":"Managing permissions (extra)"},{"location":"course_material/singularity/","text":"Learning outcomes After having completed this chapter you will be able to: Login to a remote machine with ssh Use singularity pull to convert an image from dockerhub to the \u2018singularity image format\u2019 ( .sif ) Execute a singularity container Explain the difference in default mounting behaviour between docker and singularity Use singularity shell to generate an interactive shell inside a .sif image Search and use images with both docker and singularity from bioconda Material Download the presentation Singularity documentation Singularity hub An article on Docker vs Singularity Using conda and containers with snakemake Exercises Login to remote If you are enrolled in the course, you have received an e-mail with an IP, username, private key and password. To do the Singularity exercises we will login to a remote server. Below you can find instructions on how to login. mac OS/Linux Windows Open a terminal, and cd to the directory where you have stored your private key. After that, change the file permissions of the key: chmod 400 key_<username>.pem Then, login like this: ssh -i key_<username>.pem <username>@<IP> Below you can find video tutorials and information to log in with MobaXterm. MobaXterm is an SSH client for Windows. You can use it to connect to the remote host and edit remote scripts. With MobaXterm, you will automatically login to the remote server once you\u2019ve started the SSH session. Set it up on your own computer using your own credentials and the video below. Pulling an image Singularity can take several image formats (e.g. a docker image), and convert them into it\u2019s own .sif format. Unlike docker this image doesn\u2019t live in a local image cache, but it\u2019s stored as an actual file. Exercise: On the remote server, pull the docker image that has the adjusted default CMD that we have pushed to dockerhub in this exercise ( ubuntu-figlet-df:v3 ) with singularity pull . The syntax is: singularity pull docker:// [ USER NAME ] / [ IMAGE NAME ] : [ TAG ] Answer singularity pull docker:// [ USER NAME ] /ubuntu-figlet:v3 This will result in a file called ubuntu-figlet_v3.sif Note If you weren\u2019t able to push the image in the previous exercises to your docker hub, you can use geertvangeest as username to pull the image. Executing an image These .sif files can be run as standalone executables: ./ubuntu-figlet_v3.sif Note This is shorthand for: singularity run ubuntu-figlet_v3.sif And you can overwrite the default command like this: singularity run [ IMAGE NAME ] .sif [ COMMAND ] Note In this case, you can also use ./ [ IMAGE NAME ] .sif [ COMMAND ] However, most applications require singularity run . Especially if you want to provide options like --bind (for mounting directories). Exercise: Run the .sif file without a command, and with a command that runs figlet . Do you get expected output? Do the same for the python or R image you\u2019ve created in the previous chapter. Entrypoint and singularity The daterange image has an entrypoint set, and singularity run does not overwrite it. In order to ignore both the entrypoint and cmd use singularity exec . Answer Running it without a command ( ./ubuntu-figlet_v3.sif ) should give: __ __ _ _ _ | \\/ |_ _ (_)_ __ ___ __ _ __ _ ___ __ _____ _ __| | _____| | | |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| | | | | | |_| | | | | | | | | (_| | (_| | __/ \\ V V / (_) | | | <\\__ \\_| |_| |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___| \\_/\\_/ \\___/|_| |_|\\_\\___(_) |___/ |___/ Which is the default command that we changed in the Dockerfile . Running with a another figlet command: ./ubuntu-figlet_v3.sif figlet 'Something else' Should give: ____ _ _ _ _ / ___| ___ _ __ ___ ___| |_| |__ (_)_ __ __ _ ___| |___ ___ \\___ \\ / _ \\| '_ ` _ \\ / _ \\ __| '_ \\| | '_ \\ / _` | / _ \\ / __|/ _ \\ ___) | (_) | | | | | | __/ |_| | | | | | | | (_| | | __/ \\__ \\ __/ |____/ \\___/|_| |_| |_|\\___|\\__|_| |_|_|_| |_|\\__, | \\___|_|___/\\___| |___/ R Python Pulling the search_biomart_datasets image: singularity pull docker:// [ USER NAME ] /search_biomart_datasets:v1 Running it without command: ./search_biomart_datasets_v1.sif Running with a command: ./search_biomart_datasets_v1.sif --pattern sapiens To overwrite both entrypoint and the command: singularity exec search_biomart_datasets_v1.sif search_biomart_datasets.R --pattern \"(R|r)at\" Pulling the daterange.py image: singularity pull docker:// [ USER NAME ] /daterange:v1 Running it without command: ./daterange_v1.sif Running with a command: ./daterange_v1.sif --date 20221005 To overwrite both entrypoint and the command: singularity exec daterange_v1.sif daterange.py --date 20221005 Mounting with Singularity Singularity is also different from Docker in the way it handles mounting. By default, Singularity binds your home directory and a number of paths in the root directory to the container. This results in behaviour that is almost like if you are working on the directory structure of the host. If your directory is not mounted by default It depends on the singularity settings whether most directories are mounted by default to the container. If your directory is not mounted, you can do that with the --bind option of singularity exec : singularity exec --bind /my/dir/to/mount/ [ IMAGE NAME ] .sif [ COMMAND ] Running the command pwd (full name of current working directory) will therefore result in a path on the host machine: ./ubuntu-figlet_v3.sif pwd Exercise: Run the above command. What is the output? How would the output look like if you would run a similar command with Docker? Hint A similar Docker command would look like (run this on your local computer): docker run --rm ubuntu-figlet:v3 pwd Answer The output of ./ubuntu-figlet_v3.sif pwd is the current directory on the host: i.e. /home/username if you have it in your home directory. The output of docker run --rm ubuntu-figlet:v3 pwd (on the local host) would be / , which is the default workdir (root directory) of the container. As we did not mount any host directory, this directory exists only within the container (i.e. separated from the host). Interactive shell If you want to debug or inspect an image, it can be helpful to have a shell inside the container. You can do that with singularity shell : singularity shell ubuntu-figlet_v3.sif Note To exit the shell type exit . Exercise: Can you run figlet inside this shell? Answer Yes: Singularity> figlet test _ _ | |_ ___ ___| |_ | __/ _ \\/ __| __| | || __/\\__ \\ |_ \\__\\___||___/\\__| During the lecture you have learned that singularity takes over the user privileges of the user on the host. You can get user information with command like whoami , id , groups etc. Exercise: Run the figlet container interactively. Do you have the same user privileges as if you were on the host? How is that with docker ? Answer A command like whoami will result in your username printed at stdout: Singularity> whoami myusername Singularity> id uid=1030(myusername) gid=1031(myusername) groups=1031(myusername),1001(condausers) Singularity> groups myusername condausers With singularity, you have the same privileges inside the singularity container as on the host. If you do this in the docker container (based on the same image), you\u2019ll get output like this: root@a3d6e59dc19d:/# whoami root root@a3d6e59dc19d:/# groups root root@a3d6e59dc19d:/# id uid=0(root) gid=0(root) groups=0(root) A bioinformatics example (extra) All bioconda packages also have a pre-built container. Have a look at the bioconda website , and search for fastqc . In the search results, click on the appropriate record (i.e. package \u2018fastqc\u2019). Now, scroll down and find the namespace and tag for the latest fastqc image. Now we can pull it with singularity like this: singularity pull docker://quay.io/biocontainers/fastqc:0.11.9--hdfd78af_1 Let\u2019s test the image. Download some sample reads first: mkdir reads cd reads wget https://introduction-containers.s3.eu-central-1.amazonaws.com/ecoli_reads.tar.gz tar -xzvf ecoli_reads.tar.gz rm ecoli_reads.tar.gz Now you can simply run the image as an executable preceding the commands you would like to run within the container. E.g. running fastqc would look like: cd ./fastqc_0.11.9--hdfd78af_1.sif fastqc ./reads/ecoli_*.fastq.gz This will result in html files in the directory ./reads . These are quality reports for the sequence reads. If you\u2019d like to view them, you can download them with scp or e.g. FileZilla , and view them with your local browser.","title":"Running containers with singularity"},{"location":"course_material/singularity/#learning-outcomes","text":"After having completed this chapter you will be able to: Login to a remote machine with ssh Use singularity pull to convert an image from dockerhub to the \u2018singularity image format\u2019 ( .sif ) Execute a singularity container Explain the difference in default mounting behaviour between docker and singularity Use singularity shell to generate an interactive shell inside a .sif image Search and use images with both docker and singularity from bioconda","title":"Learning outcomes"},{"location":"course_material/singularity/#material","text":"Download the presentation Singularity documentation Singularity hub An article on Docker vs Singularity Using conda and containers with snakemake","title":"Material"},{"location":"course_material/singularity/#exercises","text":"","title":"Exercises"},{"location":"course_material/singularity/#login-to-remote","text":"If you are enrolled in the course, you have received an e-mail with an IP, username, private key and password. To do the Singularity exercises we will login to a remote server. Below you can find instructions on how to login. mac OS/Linux Windows Open a terminal, and cd to the directory where you have stored your private key. After that, change the file permissions of the key: chmod 400 key_<username>.pem Then, login like this: ssh -i key_<username>.pem <username>@<IP> Below you can find video tutorials and information to log in with MobaXterm. MobaXterm is an SSH client for Windows. You can use it to connect to the remote host and edit remote scripts. With MobaXterm, you will automatically login to the remote server once you\u2019ve started the SSH session. Set it up on your own computer using your own credentials and the video below.","title":"Login to remote"},{"location":"course_material/singularity/#pulling-an-image","text":"Singularity can take several image formats (e.g. a docker image), and convert them into it\u2019s own .sif format. Unlike docker this image doesn\u2019t live in a local image cache, but it\u2019s stored as an actual file. Exercise: On the remote server, pull the docker image that has the adjusted default CMD that we have pushed to dockerhub in this exercise ( ubuntu-figlet-df:v3 ) with singularity pull . The syntax is: singularity pull docker:// [ USER NAME ] / [ IMAGE NAME ] : [ TAG ] Answer singularity pull docker:// [ USER NAME ] /ubuntu-figlet:v3 This will result in a file called ubuntu-figlet_v3.sif Note If you weren\u2019t able to push the image in the previous exercises to your docker hub, you can use geertvangeest as username to pull the image.","title":"Pulling an image"},{"location":"course_material/singularity/#executing-an-image","text":"These .sif files can be run as standalone executables: ./ubuntu-figlet_v3.sif Note This is shorthand for: singularity run ubuntu-figlet_v3.sif And you can overwrite the default command like this: singularity run [ IMAGE NAME ] .sif [ COMMAND ] Note In this case, you can also use ./ [ IMAGE NAME ] .sif [ COMMAND ] However, most applications require singularity run . Especially if you want to provide options like --bind (for mounting directories). Exercise: Run the .sif file without a command, and with a command that runs figlet . Do you get expected output? Do the same for the python or R image you\u2019ve created in the previous chapter. Entrypoint and singularity The daterange image has an entrypoint set, and singularity run does not overwrite it. In order to ignore both the entrypoint and cmd use singularity exec . Answer Running it without a command ( ./ubuntu-figlet_v3.sif ) should give: __ __ _ _ _ | \\/ |_ _ (_)_ __ ___ __ _ __ _ ___ __ _____ _ __| | _____| | | |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| | | | | | |_| | | | | | | | | (_| | (_| | __/ \\ V V / (_) | | | <\\__ \\_| |_| |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___| \\_/\\_/ \\___/|_| |_|\\_\\___(_) |___/ |___/ Which is the default command that we changed in the Dockerfile . Running with a another figlet command: ./ubuntu-figlet_v3.sif figlet 'Something else' Should give: ____ _ _ _ _ / ___| ___ _ __ ___ ___| |_| |__ (_)_ __ __ _ ___| |___ ___ \\___ \\ / _ \\| '_ ` _ \\ / _ \\ __| '_ \\| | '_ \\ / _` | / _ \\ / __|/ _ \\ ___) | (_) | | | | | | __/ |_| | | | | | | | (_| | | __/ \\__ \\ __/ |____/ \\___/|_| |_| |_|\\___|\\__|_| |_|_|_| |_|\\__, | \\___|_|___/\\___| |___/ R Python Pulling the search_biomart_datasets image: singularity pull docker:// [ USER NAME ] /search_biomart_datasets:v1 Running it without command: ./search_biomart_datasets_v1.sif Running with a command: ./search_biomart_datasets_v1.sif --pattern sapiens To overwrite both entrypoint and the command: singularity exec search_biomart_datasets_v1.sif search_biomart_datasets.R --pattern \"(R|r)at\" Pulling the daterange.py image: singularity pull docker:// [ USER NAME ] /daterange:v1 Running it without command: ./daterange_v1.sif Running with a command: ./daterange_v1.sif --date 20221005 To overwrite both entrypoint and the command: singularity exec daterange_v1.sif daterange.py --date 20221005","title":"Executing an image"},{"location":"course_material/singularity/#mounting-with-singularity","text":"Singularity is also different from Docker in the way it handles mounting. By default, Singularity binds your home directory and a number of paths in the root directory to the container. This results in behaviour that is almost like if you are working on the directory structure of the host. If your directory is not mounted by default It depends on the singularity settings whether most directories are mounted by default to the container. If your directory is not mounted, you can do that with the --bind option of singularity exec : singularity exec --bind /my/dir/to/mount/ [ IMAGE NAME ] .sif [ COMMAND ] Running the command pwd (full name of current working directory) will therefore result in a path on the host machine: ./ubuntu-figlet_v3.sif pwd Exercise: Run the above command. What is the output? How would the output look like if you would run a similar command with Docker? Hint A similar Docker command would look like (run this on your local computer): docker run --rm ubuntu-figlet:v3 pwd Answer The output of ./ubuntu-figlet_v3.sif pwd is the current directory on the host: i.e. /home/username if you have it in your home directory. The output of docker run --rm ubuntu-figlet:v3 pwd (on the local host) would be / , which is the default workdir (root directory) of the container. As we did not mount any host directory, this directory exists only within the container (i.e. separated from the host).","title":"Mounting with Singularity"},{"location":"course_material/singularity/#interactive-shell","text":"If you want to debug or inspect an image, it can be helpful to have a shell inside the container. You can do that with singularity shell : singularity shell ubuntu-figlet_v3.sif Note To exit the shell type exit . Exercise: Can you run figlet inside this shell? Answer Yes: Singularity> figlet test _ _ | |_ ___ ___| |_ | __/ _ \\/ __| __| | || __/\\__ \\ |_ \\__\\___||___/\\__| During the lecture you have learned that singularity takes over the user privileges of the user on the host. You can get user information with command like whoami , id , groups etc. Exercise: Run the figlet container interactively. Do you have the same user privileges as if you were on the host? How is that with docker ? Answer A command like whoami will result in your username printed at stdout: Singularity> whoami myusername Singularity> id uid=1030(myusername) gid=1031(myusername) groups=1031(myusername),1001(condausers) Singularity> groups myusername condausers With singularity, you have the same privileges inside the singularity container as on the host. If you do this in the docker container (based on the same image), you\u2019ll get output like this: root@a3d6e59dc19d:/# whoami root root@a3d6e59dc19d:/# groups root root@a3d6e59dc19d:/# id uid=0(root) gid=0(root) groups=0(root)","title":"Interactive shell"},{"location":"course_material/singularity/#a-bioinformatics-example-extra","text":"All bioconda packages also have a pre-built container. Have a look at the bioconda website , and search for fastqc . In the search results, click on the appropriate record (i.e. package \u2018fastqc\u2019). Now, scroll down and find the namespace and tag for the latest fastqc image. Now we can pull it with singularity like this: singularity pull docker://quay.io/biocontainers/fastqc:0.11.9--hdfd78af_1 Let\u2019s test the image. Download some sample reads first: mkdir reads cd reads wget https://introduction-containers.s3.eu-central-1.amazonaws.com/ecoli_reads.tar.gz tar -xzvf ecoli_reads.tar.gz rm ecoli_reads.tar.gz Now you can simply run the image as an executable preceding the commands you would like to run within the container. E.g. running fastqc would look like: cd ./fastqc_0.11.9--hdfd78af_1.sif fastqc ./reads/ecoli_*.fastq.gz This will result in html files in the directory ./reads . These are quality reports for the sequence reads. If you\u2019d like to view them, you can download them with scp or e.g. FileZilla , and view them with your local browser.","title":"A bioinformatics example (extra)"},{"location":"course_material/day2/guidelines/","text":"Workshop goal Over the course of the workshop, we will implement and improve a workflow to align bulk RNAseq reads to an assembly, perform some quality checks (QC), count mapped reads and identify Differentially Expressed Genes (DEG). The goal of the workshop is that after the last series of exercises, you will have implemented a simple workflow with commonly used Snakemake features. You will be able to use this workflow as a reference to implement your own workflows in the future. Software All the software required by this workflow is either: * Already installed in the snakemake conda environment * Installed in a container created during the Day 1 * Will be installed through a conda environment during today\u2019s exercises Exercises Each series of exercises is divided in multiple questions. We first provide a general explanation on the context behind each question; we then explicitly describe the task and provide details when they are required. We also provide hints that should help you with the most challenging parts of some questions. You should first try to solve the problems without using these hints! Do not hesitate to modify and overwrite your code from previous questions when specified in an exercise, as the solutions for each series of exercises are provided. If something is not clear at any point, please call us and we will do our best to answer your questions. You can also check the official Snakemake documentation","title":"General guidelines"},{"location":"course_material/day2/guidelines/#workshop-goal","text":"Over the course of the workshop, we will implement and improve a workflow to align bulk RNAseq reads to an assembly, perform some quality checks (QC), count mapped reads and identify Differentially Expressed Genes (DEG). The goal of the workshop is that after the last series of exercises, you will have implemented a simple workflow with commonly used Snakemake features. You will be able to use this workflow as a reference to implement your own workflows in the future.","title":"Workshop goal"},{"location":"course_material/day2/guidelines/#software","text":"All the software required by this workflow is either: * Already installed in the snakemake conda environment * Installed in a container created during the Day 1 * Will be installed through a conda environment during today\u2019s exercises","title":"Software"},{"location":"course_material/day2/guidelines/#exercises","text":"Each series of exercises is divided in multiple questions. We first provide a general explanation on the context behind each question; we then explicitly describe the task and provide details when they are required. We also provide hints that should help you with the most challenging parts of some questions. You should first try to solve the problems without using these hints! Do not hesitate to modify and overwrite your code from previous questions when specified in an exercise, as the solutions for each series of exercises are provided. If something is not clear at any point, please call us and we will do our best to answer your questions. You can also check the official Snakemake documentation","title":"Exercises"},{"location":"course_material/day2/introduction_snakemake/","text":"Learning outcomes After having completed this chapter you will be able to: Understand the structure of a Snakemake workflow Write rules and Snakefiles to produce the desired outputs Chain rules together Run a Snakemake workflow Exercises Command <cmd_name> not found If you try to run a command and get an error such as Command 'snakemake' not found , you are probably not in the right environment. To list them, use mamba env list . Then activate the right environment with mamba activate <env_name> . You can deactivate an environment with mamba deactivate . To list the packages installed in an environment, activate it and use mamba list . Workflow structure It is strongly advised to implement your answers in a directory called workflow (the reason for this will be explained later). You are free to chose the names and location of files for the different steps of your workflow, but we recommend that you at least group all outputs from the workflow in a results directory within the workflow directory. Creating a basic rule Rules are the basic blocks of a Snakemake workflow. A rule is like a recipe indicating how to produce a specific output ; the actual application of a rule to create an output is called a job . A rule is defined in a Snakefile with the keyword rule , and contains directives which indicate the rule\u2019s properties. We will learn about other directives later in the course. To create the simplest rule possible, we need at least two directives : - output : path of the output file for this rule - shell : shell commands to execute in order to generate the output Exercise: The following example shows the minimal syntax to implement a rule. What do you think it does? Does it create a file? If so, how is it called? rule first_step : output : 'results/first_step.txt' shell : 'echo \u201csnakemake\u201d > results/first_step.txt' Answer This rule uses the echo shell command to print the line \u201csnakemake\u201d in an output file called first_step.txt , located in the results folder. Rules are defined and written in a file called Snakefile (note the capital S and the absence of extension in the filename). This file should be located at the root of the workflow directory (here, workflow/Snakefile ). Paths in Snakemake All the paths in Snakefile are relative to the directory containing the Snakefile. Exercise: Create a Snakefile and copy the rule in it. Because the Snakemake language is basically Python, do not forget to keep the indentation as is and use space characters in the indents instead of tabs. Executing a workflow with a precise output It is now time to execute your first worklow! To do this, you need to tell Snakemake what is your target, i.e. what is the output that you want to generate. Exercise: Execute the workflow with snakemake --cores 1 <target> . What value should you use for <target>? Once Snakemake execution is finished, can you locate the output file? Answer Execute the workflow: snakemake --cores 1 results/first_step.txt Visualise your directory content: ls -alh results/ Check the output content with cat results/first_step.txt Note that during the execution of the workflow, Snakemake automatically created the missing folder ( results/ ) in the output path. If several folders are missing (for example, here, test1/test2/test3/first_step.txt ), Snakemake will create all of them . Exercise: Re-run the exact same command. What happens? Answer Nothing! You get a message saying that Snakemake did not do anything: ``` Building DAG of jobs... Nothing to be done (all requested files are present and up to date). ``` By default, Snakemake only runs a job if: * A target file explicitly requested in the `snakemake` command is missing * An intermediate file is missing and is required produce a target file * It notices input file newer than an output file, based on file modification dates. In this case, Snakemake will generate again the existing outputs. You can change this behaviour and force the re-run of a specific target by using the `-f` option: `snakemake --cores 1 -f results/first_step.txt` or force recreate ALL the outputs of the workflow using the `-F` option: `snakemake --cores 1 -F`. In practice, you can also alter Snakemake re-run policy, but we will not cover this topic in the course (see [--rerun-triggers option](https://snakemake.readthedocs.io/en/stable/executing/cli.html) in Snakemake's CLI help and [this git issue](https://github.com/snakemake/snakemake/issues/1694) for more information). In the previous example, values for these two directives are strings . For the shell directive (we will see other types of directive values later in the course), long string can be written on multiple lines for clarity, simply using a set of quotes for each line: rule first_step : output : 'results/first_step.txt' shell : 'echo \"I want to print a very very very very very very ' 'very very very very long string in my output\" > results/first_step.txt' Using the input directive The next directive used by most rules is input . Like output , input indicates the path to a file that is required by the rule to generate the output. In the following example, we modified the previous rule to use the file previously created results/first_step.tsv as an input and copy this file to results/second_step.txt : rule second_step : input : 'results/first_step.txt' output : 'results/second_step.txt' shell : 'cp results/first_step.txt results/second_step.txt' Note that with this rule definition, Snakemake will not run if results/first_step.tsv does not exist! Exercise: Modify your first rule to add an input and execute the workflow. Check that the output was created and that the files are identical. Answer Execute the workflow: snakemake --cores 1 results/second_step.txt Visualise your directory content: ls -alh results/ Check that the files are identical diff results/first_step.txt results/second_step.txt Using several rules in a workflow Creating one Snakefile per rule does not seem like a good solution, so let\u2019s try to improve this. Exercise: Delete the results/ folder, gather the two previous rules in the same Snakefile (place the first_step rule first) and try to run the workflow without specifying an output . What happens? Answer Execute the workflow without outputs: snakemake --cores 1 . When executed, Snakemake tries to generate a specific output called target , and resolves all dependencies based on this target. A target can be any output that can be generated by any rule in the workflow. When you do not specify a target, the default one is the output of the first rule in the Snakefile, here results/first_step.txt . If you had placed the second_step rule in first position, Snakemake would have crashed because the input for this rule does not exist. If you have enough time, feel free to try it! Exercise: With this in mind, use a space-separated list of targets (instead of one filename) in your command to generate multiple targets. Use the -F to force the re run of the whole workflow or delete your results/ folder beforehand. Answer Execute the workflow with multiple targets: snakemake --cores 1 -F results/first_step.txt results/second_step.txt You should now see Snakemake execute the 2 rules and produce both targets/outputs. Chaining rules Once again, writing all the outputs in the snakemake command does not look like a good solution: it is very time-consuming, error-prone and annoying! Imagine what happens when your workflow generate tens of outputs?! Fortunately, there is a way to simplify this, which relies on rules dependency. The core principle of Snakemake\u2019s execution is to compute a Directed Acyclic Graph (DAG) that summarizes dependencies between all inputs and outputs required to generate the final desired output. For each job, starting from the jobs generating the final output, Snakemake checks if the required inputs exist. If they do not, the software looks for a rule that generates the input. This process is repeated until all dependencies are resolved. This is why Snakemake is said to have a \u2018bottom-up\u2019 approach: it starts from the last outputs and go back to the first inputs. Hint Your Snakefile should look like this: rule first_step : output : 'results/first_step.txt' shell : 'echo \u201csnakemake\u201d > results/first_step.txt' rule second_step : input : 'results/first_step.txt' output : 'results/second_step.txt' shell : 'cp results/first_step.txt results/second_step.txt' Exercise: Delete the results/ folder, identify your final output and execute the workflow specifying only this output in the command. Answer Execute the workflow: snakemake --cores 1 results/second_step.txt Visualise your directory content: ls -alh results/ You should now see Snakemake execute the 2 rules and produce both outputs. To generate the output results/second_step.txt , Snakemake requires the input results/first_step.txt . Before the workflow is executed, this file does not exist, therefore, Snakemake looks for a rule that generates results/first_step.txt , in this case the first defined rule first_step . The process is then repeated for first_step . In this case, the rule does not require an input, so all dependencies are resolved, and Snakemake can generate the DAG. Important notes on chaining rules Rules produce unique outputs Because of the rules dependency process, by default, an output can only be generated by a single rule. Otherwise, Snakemake cannot decide which rule to use to generate this output, and the rules are considered ambiguous . In practice, there are ways to deal with ambiguous rules, but we will not cover them in this course (see the relevant section in the official documentation ). Rules dependency can be written more easily It is possible to refer to the output of a rule directly in another rule with the syntax rules.<rule_name>.output . Note that you don\u2019t need quotes around this statement, because it is a Snakemake object. The following example implements this syntax for the two rule defined above: rule first_step : output : 'results/first_step.txt' shell : 'echo \u201csnakemake\u201d > results/first_step.txt' rule second_step : input : rules . first_step . output output : 'results/second_step.txt' shell : 'cp results/first_step.txt results/second_step.txt' This method has several advantages, among which: * It limits the risk of error because you do not have to write the same filename at several locations * A change in output name will be automatically propagated to rules that depend on it, i.e. the name only has to be changed once * This makes the code much clearer and easier to understand: with this syntax, you instantly know the object type ( rule ), how it is created ( first_step ) and what it is ( output )","title":"Introduction to Snakemake"},{"location":"course_material/day2/introduction_snakemake/#learning-outcomes","text":"After having completed this chapter you will be able to: Understand the structure of a Snakemake workflow Write rules and Snakefiles to produce the desired outputs Chain rules together Run a Snakemake workflow","title":"Learning outcomes"},{"location":"course_material/day2/introduction_snakemake/#exercises","text":"Command <cmd_name> not found If you try to run a command and get an error such as Command 'snakemake' not found , you are probably not in the right environment. To list them, use mamba env list . Then activate the right environment with mamba activate <env_name> . You can deactivate an environment with mamba deactivate . To list the packages installed in an environment, activate it and use mamba list .","title":"Exercises"},{"location":"course_material/day2/introduction_snakemake/#workflow-structure","text":"It is strongly advised to implement your answers in a directory called workflow (the reason for this will be explained later). You are free to chose the names and location of files for the different steps of your workflow, but we recommend that you at least group all outputs from the workflow in a results directory within the workflow directory.","title":"Workflow structure"},{"location":"course_material/day2/introduction_snakemake/#creating-a-basic-rule","text":"Rules are the basic blocks of a Snakemake workflow. A rule is like a recipe indicating how to produce a specific output ; the actual application of a rule to create an output is called a job . A rule is defined in a Snakefile with the keyword rule , and contains directives which indicate the rule\u2019s properties. We will learn about other directives later in the course. To create the simplest rule possible, we need at least two directives : - output : path of the output file for this rule - shell : shell commands to execute in order to generate the output Exercise: The following example shows the minimal syntax to implement a rule. What do you think it does? Does it create a file? If so, how is it called? rule first_step : output : 'results/first_step.txt' shell : 'echo \u201csnakemake\u201d > results/first_step.txt' Answer This rule uses the echo shell command to print the line \u201csnakemake\u201d in an output file called first_step.txt , located in the results folder. Rules are defined and written in a file called Snakefile (note the capital S and the absence of extension in the filename). This file should be located at the root of the workflow directory (here, workflow/Snakefile ). Paths in Snakemake All the paths in Snakefile are relative to the directory containing the Snakefile. Exercise: Create a Snakefile and copy the rule in it. Because the Snakemake language is basically Python, do not forget to keep the indentation as is and use space characters in the indents instead of tabs.","title":"Creating a basic rule"},{"location":"course_material/day2/introduction_snakemake/#executing-a-workflow-with-a-precise-output","text":"It is now time to execute your first worklow! To do this, you need to tell Snakemake what is your target, i.e. what is the output that you want to generate. Exercise: Execute the workflow with snakemake --cores 1 <target> . What value should you use for <target>? Once Snakemake execution is finished, can you locate the output file? Answer Execute the workflow: snakemake --cores 1 results/first_step.txt Visualise your directory content: ls -alh results/ Check the output content with cat results/first_step.txt Note that during the execution of the workflow, Snakemake automatically created the missing folder ( results/ ) in the output path. If several folders are missing (for example, here, test1/test2/test3/first_step.txt ), Snakemake will create all of them . Exercise: Re-run the exact same command. What happens? Answer Nothing! You get a message saying that Snakemake did not do anything: ``` Building DAG of jobs... Nothing to be done (all requested files are present and up to date). ``` By default, Snakemake only runs a job if: * A target file explicitly requested in the `snakemake` command is missing * An intermediate file is missing and is required produce a target file * It notices input file newer than an output file, based on file modification dates. In this case, Snakemake will generate again the existing outputs. You can change this behaviour and force the re-run of a specific target by using the `-f` option: `snakemake --cores 1 -f results/first_step.txt` or force recreate ALL the outputs of the workflow using the `-F` option: `snakemake --cores 1 -F`. In practice, you can also alter Snakemake re-run policy, but we will not cover this topic in the course (see [--rerun-triggers option](https://snakemake.readthedocs.io/en/stable/executing/cli.html) in Snakemake's CLI help and [this git issue](https://github.com/snakemake/snakemake/issues/1694) for more information). In the previous example, values for these two directives are strings . For the shell directive (we will see other types of directive values later in the course), long string can be written on multiple lines for clarity, simply using a set of quotes for each line: rule first_step : output : 'results/first_step.txt' shell : 'echo \"I want to print a very very very very very very ' 'very very very very long string in my output\" > results/first_step.txt'","title":"Executing a workflow with a precise output"},{"location":"course_material/day2/introduction_snakemake/#using-the-input-directive","text":"The next directive used by most rules is input . Like output , input indicates the path to a file that is required by the rule to generate the output. In the following example, we modified the previous rule to use the file previously created results/first_step.tsv as an input and copy this file to results/second_step.txt : rule second_step : input : 'results/first_step.txt' output : 'results/second_step.txt' shell : 'cp results/first_step.txt results/second_step.txt' Note that with this rule definition, Snakemake will not run if results/first_step.tsv does not exist! Exercise: Modify your first rule to add an input and execute the workflow. Check that the output was created and that the files are identical. Answer Execute the workflow: snakemake --cores 1 results/second_step.txt Visualise your directory content: ls -alh results/ Check that the files are identical diff results/first_step.txt results/second_step.txt","title":"Using the input directive"},{"location":"course_material/day2/introduction_snakemake/#using-several-rules-in-a-workflow","text":"Creating one Snakefile per rule does not seem like a good solution, so let\u2019s try to improve this. Exercise: Delete the results/ folder, gather the two previous rules in the same Snakefile (place the first_step rule first) and try to run the workflow without specifying an output . What happens? Answer Execute the workflow without outputs: snakemake --cores 1 . When executed, Snakemake tries to generate a specific output called target , and resolves all dependencies based on this target. A target can be any output that can be generated by any rule in the workflow. When you do not specify a target, the default one is the output of the first rule in the Snakefile, here results/first_step.txt . If you had placed the second_step rule in first position, Snakemake would have crashed because the input for this rule does not exist. If you have enough time, feel free to try it! Exercise: With this in mind, use a space-separated list of targets (instead of one filename) in your command to generate multiple targets. Use the -F to force the re run of the whole workflow or delete your results/ folder beforehand. Answer Execute the workflow with multiple targets: snakemake --cores 1 -F results/first_step.txt results/second_step.txt You should now see Snakemake execute the 2 rules and produce both targets/outputs.","title":"Using several rules in a workflow"},{"location":"course_material/day2/introduction_snakemake/#chaining-rules","text":"Once again, writing all the outputs in the snakemake command does not look like a good solution: it is very time-consuming, error-prone and annoying! Imagine what happens when your workflow generate tens of outputs?! Fortunately, there is a way to simplify this, which relies on rules dependency. The core principle of Snakemake\u2019s execution is to compute a Directed Acyclic Graph (DAG) that summarizes dependencies between all inputs and outputs required to generate the final desired output. For each job, starting from the jobs generating the final output, Snakemake checks if the required inputs exist. If they do not, the software looks for a rule that generates the input. This process is repeated until all dependencies are resolved. This is why Snakemake is said to have a \u2018bottom-up\u2019 approach: it starts from the last outputs and go back to the first inputs. Hint Your Snakefile should look like this: rule first_step : output : 'results/first_step.txt' shell : 'echo \u201csnakemake\u201d > results/first_step.txt' rule second_step : input : 'results/first_step.txt' output : 'results/second_step.txt' shell : 'cp results/first_step.txt results/second_step.txt' Exercise: Delete the results/ folder, identify your final output and execute the workflow specifying only this output in the command. Answer Execute the workflow: snakemake --cores 1 results/second_step.txt Visualise your directory content: ls -alh results/ You should now see Snakemake execute the 2 rules and produce both outputs. To generate the output results/second_step.txt , Snakemake requires the input results/first_step.txt . Before the workflow is executed, this file does not exist, therefore, Snakemake looks for a rule that generates results/first_step.txt , in this case the first defined rule first_step . The process is then repeated for first_step . In this case, the rule does not require an input, so all dependencies are resolved, and Snakemake can generate the DAG.","title":"Chaining rules"},{"location":"course_material/day2/introduction_snakemake/#important-notes-on-chaining-rules","text":"","title":"Important notes on chaining rules"},{"location":"course_material/day2/introduction_snakemake/#rules-produce-unique-outputs","text":"Because of the rules dependency process, by default, an output can only be generated by a single rule. Otherwise, Snakemake cannot decide which rule to use to generate this output, and the rules are considered ambiguous . In practice, there are ways to deal with ambiguous rules, but we will not cover them in this course (see the relevant section in the official documentation ).","title":"Rules produce unique outputs"},{"location":"course_material/day2/introduction_snakemake/#rules-dependency-can-be-written-more-easily","text":"It is possible to refer to the output of a rule directly in another rule with the syntax rules.<rule_name>.output . Note that you don\u2019t need quotes around this statement, because it is a Snakemake object. The following example implements this syntax for the two rule defined above: rule first_step : output : 'results/first_step.txt' shell : 'echo \u201csnakemake\u201d > results/first_step.txt' rule second_step : input : rules . first_step . output output : 'results/second_step.txt' shell : 'cp results/first_step.txt results/second_step.txt' This method has several advantages, among which: * It limits the risk of error because you do not have to write the same filename at several locations * A change in output name will be automatically propagated to rules that depend on it, i.e. the name only has to be changed once * This makes the code much clearer and easier to understand: with this syntax, you instantly know the object type ( rule ), how it is created ( first_step ) and what it is ( output )","title":"Rules dependency can be written more easily"}]}